{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NanoGPT"
      ],
      "metadata": {
        "id": "O5-PENAR2qyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "! pip install tiktoken"
      ],
      "metadata": {
        "id": "2kSh9VgNmDQj"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import time\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import tiktoken\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "# from hellaswag import render_example, iterate_examples"
      ],
      "metadata": {
        "id": "2olmLzB12qXJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024 # max sequence length\n",
        "    vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
        "    n_layer: int = 12 # number of layers\n",
        "    n_head: int = 12 # number of heads\n",
        "    n_embd: int = 768 # embedding dimension"
      ],
      "metadata": {
        "id": "hV7Uc0ST22ai"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "        # regularization\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
        "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True) # flash attention\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "        # output projection\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "        self.gelu    = nn.GELU(approximate='tanh')\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # weight sharing scheme\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "        # init params\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            std = 0.02\n",
        "            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
        "                std *= (2 * self.config.n_layer) ** -0.5\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx is of shape (B, T)\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
        "        # forward the token and posisition embeddings\n",
        "        pos = torch.arange(0,T , dtype=torch.long, device=idx.device) # shape (T)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
        "        x = tok_emb + pos_emb\n",
        "        # forward the blocks of the transformer\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        # forward the final layernorm and the classifier\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "        # print(logits)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, device_type):\n",
        "        # start with all of the candidate parameters (that require grad)\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == \"cuda\"\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
        "        return optimizer\n"
      ],
      "metadata": {
        "id": "StDmdA8p3UxJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "print(f\"using device: {device}\")\n",
        "\n",
        "# added after video, pytorch can be serious about it's device vs. device_type distinction\n",
        "device_type = \"cuda\" if device.startswith(\"cuda\") else \"cpu\"\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(1337)\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "total_batch_size = 524288 # 2**19, ~0.5M, in number of tokens\n",
        "B = 64 # micro batch size\n",
        "T = 1024 # sequence length\n",
        "ddp_rank = 0\n",
        "ddp_world_size = 1\n",
        "master_process = True\n",
        "grad_accum_steps = total_batch_size // (B * T * ddp_world_size)\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "# create model\n",
        "model = GPT(GPTConfig(vocab_size=50304))\n",
        "# model = GPT.from_pretrained(\"gpt2\") # or init from OpenAI GPT-2\n",
        "model.to(device)\n",
        "use_compile = False # torch.compile interferes with HellaSwag eval and Generation. TODO fix\n",
        "if use_compile:\n",
        "    model = torch.compile(model)\n",
        "raw_model = model # always contains the \"raw\" unwrapped model\n",
        "\n",
        "max_lr = 6e-4\n",
        "min_lr = max_lr * 0.1\n",
        "warmup_steps = 715\n",
        "max_steps = 19073 # 19,073 steps is ~1 epoch, if data is 10B tokens and batch size 0.5M tokens\n",
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_steps:\n",
        "        return max_lr * (it+1) / warmup_steps\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > max_steps:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n",
        "    return min_lr + coeff * (max_lr - min_lr)\n",
        "\n",
        "# optimize!\n",
        "optimizer = raw_model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device_type=device_type)\n",
        "\n",
        "# create the log directory we will write checkpoints to and log to\n",
        "log_dir = \"log\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "log_file = os.path.join(log_dir, f\"log.txt\")\n",
        "with open(log_file, \"w\") as f: # open for writing to clear the file\n",
        "    pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "eixaIaF14GOT",
        "outputId": "7139a7ad-dc2d-4efa-9b2f-30c8f97b2c12"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('Input3.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "\n",
        "tokensq = enc.encode(text)\n",
        "\n",
        "with open('tokens.txt', 'w') as file:\n",
        "    file.write(' '.join(map(str, tokensq)))\n",
        "\n",
        "print(f\"Tokenized {len(tokensq)} tokens.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "dYQ4M6oddSKJ",
        "outputId": "182b86c0-463e-4761-9c30-d03e64453544"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized 12321 tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MentalDataset(Dataset):\n",
        "    def __init__(self, file_path, seq_length):\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "        # Read the tokens from the file\n",
        "        with open(file_path, 'r') as file:\n",
        "            tokens = list(map(int, file.read().split()))\n",
        "\n",
        "        # Trim tokens to ensure they can be evenly divided into sequences\n",
        "        num_sequences = len(tokens) // seq_length\n",
        "        self.tokens = tokens[:num_sequences * seq_length]\n",
        "        # print(num_sequences * seq_length, len(tokens))\n",
        "        self.tokens_tensor = torch.tensor(self.tokens, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokens_tensor) - self.seq_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Return a single sequence and its target\n",
        "        return (\n",
        "            self.tokens_tensor[idx:idx+self.seq_length],\n",
        "            self.tokens_tensor[idx+1:idx+self.seq_length+1]  # shifted by one token for the target\n",
        "        )\n",
        "\n",
        "    def next_batch(self, batch_size):\n",
        "        # Create DataLoader to yield batches\n",
        "        return DataLoader(self, batch_size=batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "id": "1n5rAvXYnhsW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'tokens.txt'\n",
        "seq_length = 1024\n",
        "\n",
        "dloader = MentalDataset(file_path, seq_length)\n",
        "\n",
        "batch_size = 5\n",
        "train_loader = dloader.next_batch(batch_size)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "CVt-SAlMnjPC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the tokens from the file\n",
        "with open('tokens.txt', 'r') as file:\n",
        "    tokens = list(map(int, file.read().split()))\n",
        "total_tokens_in_dataset = len(tokens)\n",
        "max_steps = total_tokens_in_dataset // train_loader.batch_size\n",
        "max_steps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "yu0aLOeNt6Tm",
        "outputId": "9051b065-5089-43bb-8586-4e1cbe8bb894"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2464"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "HIsaQdw02gLl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1267fa46-91cc-49a3-ed8d-68b7ede60b65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step     0 | loss: 10.971159 | lr 8.3916e-07 | norm: 19.9035 | dt: 14788.76ms | tok/sec: 35451.79\n",
            "step     1 | loss: 10.902347 | lr 1.6783e-06 | norm: 20.0064 | dt: 13569.09ms | tok/sec: 38638.40\n",
            "step     2 | loss: 10.776361 | lr 2.5175e-06 | norm: 18.9320 | dt: 13790.92ms | tok/sec: 38016.90\n",
            "step     3 | loss: 10.612269 | lr 3.3566e-06 | norm: 16.5013 | dt: 14036.78ms | tok/sec: 37351.01\n",
            "step     4 | loss: 10.416847 | lr 4.1958e-06 | norm: 13.7310 | dt: 14260.70ms | tok/sec: 36764.52\n",
            "step     5 | loss: 10.227747 | lr 5.0350e-06 | norm: 11.6115 | dt: 14479.04ms | tok/sec: 36210.15\n",
            "step     6 | loss: 10.075131 | lr 5.8741e-06 | norm: 9.9542 | dt: 14731.56ms | tok/sec: 35589.44\n",
            "step     7 | loss: 9.925721 | lr 6.7133e-06 | norm: 8.5136 | dt: 14985.94ms | tok/sec: 34985.33\n",
            "step     8 | loss: 9.788663 | lr 7.5524e-06 | norm: 7.2638 | dt: 15192.69ms | tok/sec: 34509.23\n",
            "step     9 | loss: 9.695580 | lr 8.3916e-06 | norm: 6.0587 | dt: 15436.65ms | tok/sec: 33963.84\n",
            "step    10 | loss: 9.576547 | lr 9.2308e-06 | norm: 5.0390 | dt: 15680.04ms | tok/sec: 33436.65\n",
            "step    11 | loss: 9.497281 | lr 1.0070e-05 | norm: 4.3920 | dt: 15932.90ms | tok/sec: 32906.00\n",
            "step    12 | loss: 9.426384 | lr 1.0909e-05 | norm: 3.8684 | dt: 15967.78ms | tok/sec: 32834.12\n",
            "step    13 | loss: 9.371832 | lr 1.1748e-05 | norm: 3.6080 | dt: 15871.85ms | tok/sec: 33032.57\n",
            "step    14 | loss: 9.324121 | lr 1.2587e-05 | norm: 3.3328 | dt: 15685.06ms | tok/sec: 33425.94\n",
            "step    15 | loss: 9.301770 | lr 1.3427e-05 | norm: 3.1132 | dt: 15703.30ms | tok/sec: 33387.12\n",
            "step    16 | loss: 9.274096 | lr 1.4266e-05 | norm: 2.9238 | dt: 15742.05ms | tok/sec: 33304.95\n",
            "step    17 | loss: 9.218679 | lr 1.5105e-05 | norm: 2.8936 | dt: 15815.38ms | tok/sec: 33150.52\n",
            "step    18 | loss: 9.214578 | lr 1.5944e-05 | norm: 2.8193 | dt: 15849.47ms | tok/sec: 33079.22\n",
            "step    19 | loss: 9.185292 | lr 1.6783e-05 | norm: 2.8695 | dt: 15832.40ms | tok/sec: 33114.87\n",
            "step    20 | loss: 9.175320 | lr 1.7622e-05 | norm: 2.8014 | dt: 15745.10ms | tok/sec: 33298.49\n",
            "step    21 | loss: 9.127151 | lr 1.8462e-05 | norm: 2.8080 | dt: 15745.22ms | tok/sec: 33298.24\n",
            "step    22 | loss: 9.084509 | lr 1.9301e-05 | norm: 2.8255 | dt: 15713.83ms | tok/sec: 33364.74\n",
            "step    23 | loss: 9.075878 | lr 2.0140e-05 | norm: 2.7867 | dt: 15821.97ms | tok/sec: 33136.71\n",
            "step    24 | loss: 9.019591 | lr 2.0979e-05 | norm: 2.8387 | dt: 15843.72ms | tok/sec: 33091.21\n",
            "step    25 | loss: 8.981112 | lr 2.1818e-05 | norm: 2.9081 | dt: 15863.61ms | tok/sec: 33049.73\n",
            "step    26 | loss: 8.894112 | lr 2.2657e-05 | norm: 3.0077 | dt: 15883.55ms | tok/sec: 33008.23\n",
            "step    27 | loss: 8.831996 | lr 2.3497e-05 | norm: 3.0343 | dt: 15838.16ms | tok/sec: 33102.84\n",
            "step    28 | loss: 8.759927 | lr 2.4336e-05 | norm: 4.9573 | dt: 15820.17ms | tok/sec: 33140.47\n",
            "step    29 | loss: 8.693052 | lr 2.5175e-05 | norm: 4.5086 | dt: 15779.54ms | tok/sec: 33225.80\n",
            "step    30 | loss: 8.644876 | lr 2.6014e-05 | norm: 4.4960 | dt: 15798.77ms | tok/sec: 33185.36\n",
            "step    31 | loss: 8.560237 | lr 2.6853e-05 | norm: 4.2183 | dt: 15845.08ms | tok/sec: 33088.37\n",
            "step    32 | loss: 8.483358 | lr 2.7692e-05 | norm: 3.4602 | dt: 15888.19ms | tok/sec: 32998.61\n",
            "step    33 | loss: 8.448545 | lr 2.8531e-05 | norm: 4.3642 | dt: 15889.54ms | tok/sec: 32995.79\n",
            "step    34 | loss: 8.382154 | lr 2.9371e-05 | norm: 3.8882 | dt: 15906.04ms | tok/sec: 32961.57\n",
            "step    35 | loss: 8.293408 | lr 3.0210e-05 | norm: 3.5085 | dt: 15871.86ms | tok/sec: 33032.55\n",
            "step    36 | loss: 8.263041 | lr 3.1049e-05 | norm: 5.2491 | dt: 15798.03ms | tok/sec: 33186.91\n",
            "step    37 | loss: 8.179032 | lr 3.1888e-05 | norm: 3.6249 | dt: 15783.52ms | tok/sec: 33217.43\n",
            "step    38 | loss: 8.112743 | lr 3.2727e-05 | norm: 3.4256 | dt: 15774.50ms | tok/sec: 33236.43\n",
            "step    39 | loss: 8.046350 | lr 3.3566e-05 | norm: 4.6380 | dt: 15801.39ms | tok/sec: 33179.86\n",
            "step    40 | loss: 7.985067 | lr 3.4406e-05 | norm: 3.3174 | dt: 15853.08ms | tok/sec: 33071.68\n",
            "step    41 | loss: 7.936624 | lr 3.5245e-05 | norm: 3.7636 | dt: 15841.34ms | tok/sec: 33096.19\n",
            "step    42 | loss: 7.867174 | lr 3.6084e-05 | norm: 5.6459 | dt: 15872.43ms | tok/sec: 33031.37\n",
            "step    43 | loss: 7.785375 | lr 3.6923e-05 | norm: 5.2753 | dt: 15834.65ms | tok/sec: 33110.17\n",
            "step    44 | loss: 7.714753 | lr 3.7762e-05 | norm: 5.4337 | dt: 15882.09ms | tok/sec: 33011.28\n",
            "step    45 | loss: 7.637531 | lr 3.8601e-05 | norm: 3.2683 | dt: 15880.50ms | tok/sec: 33014.59\n",
            "step    46 | loss: 7.546081 | lr 3.9441e-05 | norm: 10.3993 | dt: 15873.55ms | tok/sec: 33029.04\n",
            "step    47 | loss: 7.567101 | lr 4.0280e-05 | norm: 4.2495 | dt: 15874.34ms | tok/sec: 33027.39\n",
            "step    48 | loss: 7.518742 | lr 4.1119e-05 | norm: 5.0379 | dt: 15883.23ms | tok/sec: 33008.91\n",
            "step    49 | loss: 7.402120 | lr 4.1958e-05 | norm: 3.8954 | dt: 15861.34ms | tok/sec: 33054.45\n",
            "step    50 | loss: 7.380445 | lr 4.2797e-05 | norm: 6.6048 | dt: 15820.66ms | tok/sec: 33139.45\n",
            "step    51 | loss: 7.322564 | lr 4.3636e-05 | norm: 6.5775 | dt: 15819.15ms | tok/sec: 33142.62\n",
            "step    52 | loss: 7.251576 | lr 4.4476e-05 | norm: 8.1129 | dt: 15858.95ms | tok/sec: 33059.43\n",
            "step    53 | loss: 7.154581 | lr 4.5315e-05 | norm: 6.3794 | dt: 15778.79ms | tok/sec: 33227.38\n",
            "step    54 | loss: 7.104165 | lr 4.6154e-05 | norm: 5.2748 | dt: 15825.38ms | tok/sec: 33129.57\n",
            "step    55 | loss: 7.037647 | lr 4.6993e-05 | norm: 6.9977 | dt: 15832.04ms | tok/sec: 33115.63\n",
            "step    56 | loss: 6.996002 | lr 4.7832e-05 | norm: 6.4009 | dt: 15873.98ms | tok/sec: 33028.13\n",
            "step    57 | loss: 6.890357 | lr 4.8671e-05 | norm: 4.2491 | dt: 15863.59ms | tok/sec: 33049.77\n",
            "step    58 | loss: 6.797448 | lr 4.9510e-05 | norm: 4.1899 | dt: 15856.48ms | tok/sec: 33064.60\n",
            "step    59 | loss: 6.780754 | lr 5.0350e-05 | norm: 5.1581 | dt: 15851.20ms | tok/sec: 33075.60\n",
            "step    60 | loss: 6.696309 | lr 5.1189e-05 | norm: 5.5192 | dt: 15848.25ms | tok/sec: 33081.76\n",
            "step    61 | loss: 6.634416 | lr 5.2028e-05 | norm: 6.0408 | dt: 15836.99ms | tok/sec: 33105.29\n",
            "step    62 | loss: 6.581562 | lr 5.2867e-05 | norm: 6.1950 | dt: 15893.22ms | tok/sec: 32988.16\n",
            "step    63 | loss: 6.428403 | lr 5.3706e-05 | norm: 4.0368 | dt: 15870.04ms | tok/sec: 33036.33\n",
            "step    64 | loss: 6.416371 | lr 5.4545e-05 | norm: 7.2855 | dt: 15868.94ms | tok/sec: 33038.63\n",
            "step    65 | loss: 6.343579 | lr 5.5385e-05 | norm: 6.6505 | dt: 15863.62ms | tok/sec: 33049.70\n",
            "step    66 | loss: 6.245628 | lr 5.6224e-05 | norm: 7.0371 | dt: 15881.59ms | tok/sec: 33012.32\n",
            "step    67 | loss: 6.254013 | lr 5.7063e-05 | norm: 7.3835 | dt: 15886.84ms | tok/sec: 33001.40\n",
            "step    68 | loss: 6.131016 | lr 5.7902e-05 | norm: 5.4259 | dt: 15874.55ms | tok/sec: 33026.96\n",
            "step    69 | loss: 6.067389 | lr 5.8741e-05 | norm: 6.7931 | dt: 15867.52ms | tok/sec: 33041.58\n",
            "step    70 | loss: 6.022913 | lr 5.9580e-05 | norm: 4.8830 | dt: 15881.01ms | tok/sec: 33013.52\n",
            "step    71 | loss: 5.891543 | lr 6.0420e-05 | norm: 6.2322 | dt: 15861.56ms | tok/sec: 33054.00\n",
            "step    72 | loss: 5.873769 | lr 6.1259e-05 | norm: 5.1534 | dt: 15874.67ms | tok/sec: 33026.71\n",
            "step    73 | loss: 5.849735 | lr 6.2098e-05 | norm: 5.2457 | dt: 15888.30ms | tok/sec: 32998.36\n",
            "step    74 | loss: 5.738869 | lr 6.2937e-05 | norm: 6.5995 | dt: 15842.72ms | tok/sec: 33093.30\n",
            "step    75 | loss: 5.728487 | lr 6.3776e-05 | norm: 6.6665 | dt: 15880.69ms | tok/sec: 33014.19\n",
            "step    76 | loss: 5.657668 | lr 6.4615e-05 | norm: 6.5380 | dt: 15896.30ms | tok/sec: 32981.76\n",
            "step    77 | loss: 5.532390 | lr 6.5455e-05 | norm: 5.5703 | dt: 15873.65ms | tok/sec: 33028.83\n",
            "step    78 | loss: 5.544458 | lr 6.6294e-05 | norm: 5.6983 | dt: 15880.51ms | tok/sec: 33014.56\n",
            "step    79 | loss: 5.410718 | lr 6.7133e-05 | norm: 8.1106 | dt: 15890.59ms | tok/sec: 32993.60\n",
            "step    80 | loss: 5.390651 | lr 6.7972e-05 | norm: 5.2172 | dt: 15881.48ms | tok/sec: 33012.53\n",
            "step    81 | loss: 5.350705 | lr 6.8811e-05 | norm: 8.0038 | dt: 15863.49ms | tok/sec: 33049.97\n",
            "step    82 | loss: 5.292392 | lr 6.9650e-05 | norm: 7.8208 | dt: 15885.64ms | tok/sec: 33003.89\n",
            "step    83 | loss: 5.262376 | lr 7.0490e-05 | norm: 9.6422 | dt: 15866.21ms | tok/sec: 33044.30\n",
            "step    84 | loss: 5.150659 | lr 7.1329e-05 | norm: 4.5319 | dt: 15883.65ms | tok/sec: 33008.03\n",
            "step    85 | loss: 5.056139 | lr 7.2168e-05 | norm: 7.4804 | dt: 15902.87ms | tok/sec: 32968.13\n",
            "step    86 | loss: 5.045391 | lr 7.3007e-05 | norm: 5.0958 | dt: 15862.00ms | tok/sec: 33053.07\n",
            "step    87 | loss: 4.968912 | lr 7.3846e-05 | norm: 7.1497 | dt: 15880.77ms | tok/sec: 33014.02\n",
            "step    88 | loss: 4.936476 | lr 7.4685e-05 | norm: 7.6409 | dt: 15882.63ms | tok/sec: 33010.16\n",
            "step    89 | loss: 4.909523 | lr 7.5524e-05 | norm: 6.3527 | dt: 15883.88ms | tok/sec: 33007.56\n",
            "step    90 | loss: 4.761952 | lr 7.6364e-05 | norm: 5.9639 | dt: 15878.22ms | tok/sec: 33019.32\n",
            "step    91 | loss: 4.732116 | lr 7.7203e-05 | norm: 5.6350 | dt: 15896.85ms | tok/sec: 32980.62\n",
            "step    92 | loss: 4.725848 | lr 7.8042e-05 | norm: 8.4009 | dt: 15878.86ms | tok/sec: 33017.99\n",
            "step    93 | loss: 4.676796 | lr 7.8881e-05 | norm: 5.6935 | dt: 15895.87ms | tok/sec: 32982.65\n",
            "step    94 | loss: 4.596690 | lr 7.9720e-05 | norm: 5.7531 | dt: 15866.11ms | tok/sec: 33044.51\n",
            "step    95 | loss: 4.620295 | lr 8.0559e-05 | norm: 4.6929 | dt: 15870.41ms | tok/sec: 33035.56\n",
            "step    96 | loss: 4.481619 | lr 8.1399e-05 | norm: 5.0558 | dt: 15861.99ms | tok/sec: 33053.11\n",
            "step    97 | loss: 4.457491 | lr 8.2238e-05 | norm: 5.4509 | dt: 15893.93ms | tok/sec: 32986.68\n",
            "step    98 | loss: 4.370028 | lr 8.3077e-05 | norm: 5.1510 | dt: 15854.41ms | tok/sec: 33068.90\n",
            "step    99 | loss: 4.293824 | lr 8.3916e-05 | norm: 5.4925 | dt: 15866.54ms | tok/sec: 33043.63\n",
            "step   100 | loss: 4.215219 | lr 8.4755e-05 | norm: 6.2944 | dt: 15871.37ms | tok/sec: 33033.56\n",
            "step   101 | loss: 4.264829 | lr 8.5594e-05 | norm: 5.9546 | dt: 15891.21ms | tok/sec: 32992.33\n",
            "step   102 | loss: 4.173769 | lr 8.6434e-05 | norm: 5.2011 | dt: 15846.27ms | tok/sec: 33085.89\n",
            "step   103 | loss: 4.060437 | lr 8.7273e-05 | norm: 5.6945 | dt: 15875.07ms | tok/sec: 33025.88\n",
            "step   104 | loss: 4.082192 | lr 8.8112e-05 | norm: 5.9206 | dt: 15879.68ms | tok/sec: 33016.28\n",
            "step   105 | loss: 4.045882 | lr 8.8951e-05 | norm: 6.0483 | dt: 15871.55ms | tok/sec: 33033.19\n",
            "step   106 | loss: 3.964471 | lr 8.9790e-05 | norm: 5.6397 | dt: 15874.23ms | tok/sec: 33027.62\n",
            "step   107 | loss: 3.992279 | lr 9.0629e-05 | norm: 5.7323 | dt: 15881.83ms | tok/sec: 33011.81\n",
            "step   108 | loss: 3.907383 | lr 9.1469e-05 | norm: 6.1473 | dt: 15882.59ms | tok/sec: 33010.24\n",
            "step   109 | loss: 3.801967 | lr 9.2308e-05 | norm: 6.7207 | dt: 15880.64ms | tok/sec: 33014.29\n",
            "step   110 | loss: 3.733594 | lr 9.3147e-05 | norm: 4.6528 | dt: 15898.29ms | tok/sec: 32977.64\n",
            "step   111 | loss: 3.693041 | lr 9.3986e-05 | norm: 7.5237 | dt: 15870.52ms | tok/sec: 33035.33\n",
            "step   112 | loss: 3.649524 | lr 9.4825e-05 | norm: 6.7754 | dt: 15866.79ms | tok/sec: 33043.10\n",
            "step   113 | loss: 3.648057 | lr 9.5664e-05 | norm: 6.7437 | dt: 15876.66ms | tok/sec: 33022.56\n",
            "step   114 | loss: 3.652197 | lr 9.6503e-05 | norm: 6.9316 | dt: 15893.82ms | tok/sec: 32986.92\n",
            "step   115 | loss: 3.557091 | lr 9.7343e-05 | norm: 5.1160 | dt: 15892.02ms | tok/sec: 32990.64\n",
            "step   116 | loss: 3.481959 | lr 9.8182e-05 | norm: 5.8444 | dt: 15887.13ms | tok/sec: 33000.81\n",
            "step   117 | loss: 3.476777 | lr 9.9021e-05 | norm: 8.1354 | dt: 15880.39ms | tok/sec: 33014.81\n",
            "step   118 | loss: 3.381010 | lr 9.9860e-05 | norm: 6.3990 | dt: 15882.16ms | tok/sec: 33011.13\n",
            "step   119 | loss: 3.412408 | lr 1.0070e-04 | norm: 7.8982 | dt: 15901.63ms | tok/sec: 32970.72\n",
            "step   120 | loss: 3.320063 | lr 1.0154e-04 | norm: 6.4550 | dt: 15881.51ms | tok/sec: 33012.49\n",
            "step   121 | loss: 3.341990 | lr 1.0238e-04 | norm: 6.9159 | dt: 15881.77ms | tok/sec: 33011.95\n",
            "step   122 | loss: 3.234677 | lr 1.0322e-04 | norm: 7.7002 | dt: 15880.64ms | tok/sec: 33014.29\n",
            "step   123 | loss: 3.234310 | lr 1.0406e-04 | norm: 5.5440 | dt: 15901.77ms | tok/sec: 32970.42\n",
            "step   124 | loss: 3.200607 | lr 1.0490e-04 | norm: 6.7344 | dt: 15886.19ms | tok/sec: 33002.75\n",
            "step   125 | loss: 3.140108 | lr 1.0573e-04 | norm: 6.2631 | dt: 15888.41ms | tok/sec: 32998.15\n",
            "step   126 | loss: 3.062785 | lr 1.0657e-04 | norm: 6.7791 | dt: 15886.13ms | tok/sec: 33002.89\n",
            "step   127 | loss: 3.125689 | lr 1.0741e-04 | norm: 7.0579 | dt: 15898.90ms | tok/sec: 32976.37\n",
            "step   128 | loss: 3.032678 | lr 1.0825e-04 | norm: 5.5408 | dt: 15885.17ms | tok/sec: 33004.87\n",
            "step   129 | loss: 2.964708 | lr 1.0909e-04 | norm: 5.7182 | dt: 15875.61ms | tok/sec: 33024.74\n",
            "step   130 | loss: 2.989936 | lr 1.0993e-04 | norm: 6.8083 | dt: 15887.43ms | tok/sec: 33000.17\n",
            "step   131 | loss: 2.892332 | lr 1.1077e-04 | norm: 5.7494 | dt: 15894.93ms | tok/sec: 32984.60\n",
            "step   132 | loss: 2.848974 | lr 1.1161e-04 | norm: 6.3516 | dt: 15835.31ms | tok/sec: 33108.79\n",
            "step   133 | loss: 2.911486 | lr 1.1245e-04 | norm: 6.6819 | dt: 15884.80ms | tok/sec: 33005.63\n",
            "step   134 | loss: 2.767230 | lr 1.1329e-04 | norm: 8.9620 | dt: 15881.94ms | tok/sec: 33011.59\n",
            "step   135 | loss: 2.726514 | lr 1.1413e-04 | norm: 5.5388 | dt: 15886.77ms | tok/sec: 33001.55\n",
            "step   136 | loss: 2.836513 | lr 1.1497e-04 | norm: 8.3047 | dt: 15873.44ms | tok/sec: 33029.26\n",
            "step   137 | loss: 2.715899 | lr 1.1580e-04 | norm: 8.3571 | dt: 15847.99ms | tok/sec: 33082.30\n",
            "step   138 | loss: 2.791139 | lr 1.1664e-04 | norm: 7.0714 | dt: 15858.89ms | tok/sec: 33059.56\n",
            "step   139 | loss: 2.699458 | lr 1.1748e-04 | norm: 8.1403 | dt: 15872.11ms | tok/sec: 33032.02\n",
            "step   140 | loss: 2.638672 | lr 1.1832e-04 | norm: 7.2554 | dt: 15897.80ms | tok/sec: 32978.65\n",
            "step   141 | loss: 2.671681 | lr 1.1916e-04 | norm: 6.2478 | dt: 15884.83ms | tok/sec: 33005.57\n",
            "step   142 | loss: 2.552842 | lr 1.2000e-04 | norm: 6.3389 | dt: 15865.85ms | tok/sec: 33045.06\n",
            "step   143 | loss: 2.546563 | lr 1.2084e-04 | norm: 5.9205 | dt: 15882.54ms | tok/sec: 33010.33\n",
            "step   144 | loss: 2.522622 | lr 1.2168e-04 | norm: 5.8601 | dt: 15896.24ms | tok/sec: 32981.89\n",
            "step   145 | loss: 2.521591 | lr 1.2252e-04 | norm: 7.3421 | dt: 15892.86ms | tok/sec: 32988.90\n",
            "step   146 | loss: 2.485780 | lr 1.2336e-04 | norm: 6.8096 | dt: 15876.89ms | tok/sec: 33022.09\n",
            "step   147 | loss: 2.472146 | lr 1.2420e-04 | norm: 5.4752 | dt: 15882.71ms | tok/sec: 33009.98\n",
            "step   148 | loss: 2.382334 | lr 1.2503e-04 | norm: 7.3356 | dt: 15889.14ms | tok/sec: 32996.63\n",
            "step   149 | loss: 2.349661 | lr 1.2587e-04 | norm: 5.2011 | dt: 15856.80ms | tok/sec: 33063.92\n",
            "step   150 | loss: 2.330683 | lr 1.2671e-04 | norm: 6.0704 | dt: 15862.35ms | tok/sec: 33052.36\n",
            "step   151 | loss: 2.311579 | lr 1.2755e-04 | norm: 5.0342 | dt: 15879.51ms | tok/sec: 33016.64\n",
            "step   152 | loss: 2.359183 | lr 1.2839e-04 | norm: 5.8721 | dt: 15890.84ms | tok/sec: 32993.10\n",
            "step   153 | loss: 2.253323 | lr 1.2923e-04 | norm: 5.4865 | dt: 15875.43ms | tok/sec: 33025.12\n",
            "step   154 | loss: 2.210804 | lr 1.3007e-04 | norm: 4.8923 | dt: 15882.31ms | tok/sec: 33010.81\n",
            "step   155 | loss: 2.206570 | lr 1.3091e-04 | norm: 5.6862 | dt: 15880.24ms | tok/sec: 33015.12\n",
            "step   156 | loss: 2.163060 | lr 1.3175e-04 | norm: 5.0880 | dt: 15867.18ms | tok/sec: 33042.29\n",
            "step   157 | loss: 2.177498 | lr 1.3259e-04 | norm: 6.5054 | dt: 15861.03ms | tok/sec: 33055.10\n",
            "step   158 | loss: 2.141972 | lr 1.3343e-04 | norm: 4.9142 | dt: 15882.55ms | tok/sec: 33010.31\n",
            "step   159 | loss: 2.113432 | lr 1.3427e-04 | norm: 5.2878 | dt: 15879.75ms | tok/sec: 33016.13\n",
            "step   160 | loss: 2.049571 | lr 1.3510e-04 | norm: 4.4906 | dt: 15884.24ms | tok/sec: 33006.81\n",
            "step   161 | loss: 2.025865 | lr 1.3594e-04 | norm: 4.9420 | dt: 15861.21ms | tok/sec: 33054.72\n",
            "step   162 | loss: 2.035703 | lr 1.3678e-04 | norm: 5.1009 | dt: 15870.84ms | tok/sec: 33034.67\n",
            "step   163 | loss: 2.002524 | lr 1.3762e-04 | norm: 4.7111 | dt: 15873.72ms | tok/sec: 33028.68\n",
            "step   164 | loss: 2.051881 | lr 1.3846e-04 | norm: 5.9972 | dt: 15890.93ms | tok/sec: 32992.92\n",
            "step   165 | loss: 1.991747 | lr 1.3930e-04 | norm: 4.7602 | dt: 15892.22ms | tok/sec: 32990.23\n",
            "step   166 | loss: 2.023652 | lr 1.4014e-04 | norm: 4.3651 | dt: 15896.86ms | tok/sec: 32980.59\n",
            "step   167 | loss: 1.916380 | lr 1.4098e-04 | norm: 4.5014 | dt: 15884.24ms | tok/sec: 33006.81\n",
            "step   168 | loss: 2.013476 | lr 1.4182e-04 | norm: 4.5311 | dt: 15880.71ms | tok/sec: 33014.15\n",
            "step   169 | loss: 1.903389 | lr 1.4266e-04 | norm: 5.2535 | dt: 15885.03ms | tok/sec: 33005.16\n",
            "step   170 | loss: 1.902186 | lr 1.4350e-04 | norm: 5.3608 | dt: 15866.20ms | tok/sec: 33044.34\n",
            "step   171 | loss: 1.847950 | lr 1.4434e-04 | norm: 4.6693 | dt: 15855.35ms | tok/sec: 33066.95\n",
            "step   172 | loss: 1.839209 | lr 1.4517e-04 | norm: 4.6237 | dt: 15870.15ms | tok/sec: 33036.11\n",
            "step   173 | loss: 1.791128 | lr 1.4601e-04 | norm: 3.6944 | dt: 15881.17ms | tok/sec: 33013.18\n",
            "step   174 | loss: 1.825096 | lr 1.4685e-04 | norm: 4.4050 | dt: 15897.52ms | tok/sec: 32979.23\n",
            "step   175 | loss: 1.748772 | lr 1.4769e-04 | norm: 4.4688 | dt: 15869.14ms | tok/sec: 33038.20\n",
            "step   176 | loss: 1.757536 | lr 1.4853e-04 | norm: 4.0826 | dt: 15877.75ms | tok/sec: 33020.30\n",
            "step   177 | loss: 1.723379 | lr 1.4937e-04 | norm: 4.0268 | dt: 15855.02ms | tok/sec: 33067.63\n",
            "step   178 | loss: 1.698069 | lr 1.5021e-04 | norm: 3.9939 | dt: 15881.83ms | tok/sec: 33011.82\n",
            "step   179 | loss: 1.648855 | lr 1.5105e-04 | norm: 4.4029 | dt: 15852.07ms | tok/sec: 33073.78\n",
            "step   180 | loss: 1.652560 | lr 1.5189e-04 | norm: 4.1764 | dt: 15865.03ms | tok/sec: 33046.77\n",
            "step   181 | loss: 1.641612 | lr 1.5273e-04 | norm: 3.8613 | dt: 15847.76ms | tok/sec: 33082.77\n",
            "step   182 | loss: 1.563297 | lr 1.5357e-04 | norm: 3.9158 | dt: 15867.55ms | tok/sec: 33041.52\n",
            "step   183 | loss: 1.619066 | lr 1.5441e-04 | norm: 3.8022 | dt: 15852.91ms | tok/sec: 33072.04\n",
            "step   184 | loss: 1.596398 | lr 1.5524e-04 | norm: 4.2788 | dt: 15882.81ms | tok/sec: 33009.79\n",
            "step   185 | loss: 1.556883 | lr 1.5608e-04 | norm: 3.4478 | dt: 15879.86ms | tok/sec: 33015.90\n",
            "step   186 | loss: 1.521300 | lr 1.5692e-04 | norm: 4.3383 | dt: 15888.28ms | tok/sec: 32998.41\n",
            "step   187 | loss: 1.524928 | lr 1.5776e-04 | norm: 2.9136 | dt: 15860.64ms | tok/sec: 33055.91\n",
            "step   188 | loss: 1.506928 | lr 1.5860e-04 | norm: 3.3760 | dt: 15877.88ms | tok/sec: 33020.03\n",
            "step   189 | loss: 1.503475 | lr 1.5944e-04 | norm: 3.3027 | dt: 15871.74ms | tok/sec: 33032.79\n",
            "step   190 | loss: 1.532140 | lr 1.6028e-04 | norm: 3.5500 | dt: 15837.14ms | tok/sec: 33104.97\n",
            "step   191 | loss: 1.477736 | lr 1.6112e-04 | norm: 3.3254 | dt: 15877.18ms | tok/sec: 33021.49\n",
            "step   192 | loss: 1.477523 | lr 1.6196e-04 | norm: 4.0188 | dt: 15883.53ms | tok/sec: 33008.28\n",
            "step   193 | loss: 1.435372 | lr 1.6280e-04 | norm: 3.7548 | dt: 15884.67ms | tok/sec: 33005.92\n",
            "step   194 | loss: 1.414471 | lr 1.6364e-04 | norm: 3.6603 | dt: 15856.62ms | tok/sec: 33064.30\n",
            "step   195 | loss: 1.378944 | lr 1.6448e-04 | norm: 3.6930 | dt: 15836.95ms | tok/sec: 33105.36\n",
            "step   196 | loss: 1.370934 | lr 1.6531e-04 | norm: 3.5074 | dt: 15886.08ms | tok/sec: 33002.97\n",
            "step   197 | loss: 1.389271 | lr 1.6615e-04 | norm: 3.5741 | dt: 15858.07ms | tok/sec: 33061.27\n",
            "step   198 | loss: 1.358711 | lr 1.6699e-04 | norm: 3.2837 | dt: 15889.20ms | tok/sec: 32996.50\n",
            "step   199 | loss: 1.321519 | lr 1.6783e-04 | norm: 3.2507 | dt: 15889.64ms | tok/sec: 32995.58\n",
            "step   200 | loss: 1.308072 | lr 1.6867e-04 | norm: 3.0455 | dt: 15888.23ms | tok/sec: 32998.52\n",
            "step   201 | loss: 1.258488 | lr 1.6951e-04 | norm: 2.7891 | dt: 15890.18ms | tok/sec: 32994.46\n",
            "step   202 | loss: 1.265776 | lr 1.7035e-04 | norm: 2.9886 | dt: 15861.45ms | tok/sec: 33054.23\n",
            "step   203 | loss: 1.296504 | lr 1.7119e-04 | norm: 2.8154 | dt: 15882.30ms | tok/sec: 33010.84\n",
            "step   204 | loss: 1.296191 | lr 1.7203e-04 | norm: 3.0877 | dt: 15900.11ms | tok/sec: 32973.87\n",
            "step   205 | loss: 1.237239 | lr 1.7287e-04 | norm: 3.0973 | dt: 15880.26ms | tok/sec: 33015.08\n",
            "step   206 | loss: 1.209495 | lr 1.7371e-04 | norm: 2.5686 | dt: 15879.10ms | tok/sec: 33017.50\n",
            "step   207 | loss: 1.261784 | lr 1.7455e-04 | norm: 3.5659 | dt: 15887.55ms | tok/sec: 32999.92\n",
            "step   208 | loss: 1.234708 | lr 1.7538e-04 | norm: 2.7703 | dt: 15896.71ms | tok/sec: 32980.91\n",
            "step   209 | loss: 1.206202 | lr 1.7622e-04 | norm: 2.6327 | dt: 15887.60ms | tok/sec: 32999.82\n",
            "step   210 | loss: 1.222379 | lr 1.7706e-04 | norm: 3.2875 | dt: 15885.07ms | tok/sec: 33005.08\n",
            "step   211 | loss: 1.161730 | lr 1.7790e-04 | norm: 2.9417 | dt: 15853.80ms | tok/sec: 33070.17\n",
            "step   212 | loss: 1.179687 | lr 1.7874e-04 | norm: 3.0182 | dt: 15895.66ms | tok/sec: 32983.08\n",
            "step   213 | loss: 1.148114 | lr 1.7958e-04 | norm: 3.0255 | dt: 15867.08ms | tok/sec: 33042.49\n",
            "step   214 | loss: 1.152430 | lr 1.8042e-04 | norm: 2.9432 | dt: 15877.46ms | tok/sec: 33020.90\n",
            "step   215 | loss: 1.135471 | lr 1.8126e-04 | norm: 2.6845 | dt: 15888.49ms | tok/sec: 32997.97\n",
            "step   216 | loss: 1.098616 | lr 1.8210e-04 | norm: 2.5777 | dt: 15883.91ms | tok/sec: 33007.50\n",
            "step   217 | loss: 1.146320 | lr 1.8294e-04 | norm: 3.2063 | dt: 15895.49ms | tok/sec: 32983.45\n",
            "step   218 | loss: 1.119405 | lr 1.8378e-04 | norm: 2.5305 | dt: 15877.12ms | tok/sec: 33021.61\n",
            "step   219 | loss: 1.101861 | lr 1.8462e-04 | norm: 3.3229 | dt: 15857.78ms | tok/sec: 33061.88\n",
            "step   220 | loss: 1.048918 | lr 1.8545e-04 | norm: 2.6864 | dt: 15879.95ms | tok/sec: 33015.72\n",
            "step   221 | loss: 1.046860 | lr 1.8629e-04 | norm: 2.7497 | dt: 15889.97ms | tok/sec: 32994.90\n",
            "step   222 | loss: 1.039713 | lr 1.8713e-04 | norm: 2.3709 | dt: 15886.75ms | tok/sec: 33001.59\n",
            "step   223 | loss: 1.012330 | lr 1.8797e-04 | norm: 2.4962 | dt: 15882.38ms | tok/sec: 33010.66\n",
            "step   224 | loss: 1.023216 | lr 1.8881e-04 | norm: 2.3408 | dt: 15887.78ms | tok/sec: 32999.46\n",
            "step   225 | loss: 1.035104 | lr 1.8965e-04 | norm: 2.3829 | dt: 15900.91ms | tok/sec: 32972.21\n",
            "step   226 | loss: 0.996719 | lr 1.9049e-04 | norm: 2.9947 | dt: 15882.08ms | tok/sec: 33011.30\n",
            "step   227 | loss: 0.954871 | lr 1.9133e-04 | norm: 2.6482 | dt: 15884.51ms | tok/sec: 33006.24\n",
            "step   228 | loss: 1.009795 | lr 1.9217e-04 | norm: 3.1845 | dt: 15882.56ms | tok/sec: 33010.30\n",
            "step   229 | loss: 0.965413 | lr 1.9301e-04 | norm: 2.5793 | dt: 15875.56ms | tok/sec: 33024.85\n",
            "step   230 | loss: 0.932982 | lr 1.9385e-04 | norm: 2.2319 | dt: 15888.64ms | tok/sec: 32997.66\n",
            "step   231 | loss: 0.904541 | lr 1.9469e-04 | norm: 2.1914 | dt: 15854.42ms | tok/sec: 33068.89\n",
            "step   232 | loss: 0.966251 | lr 1.9552e-04 | norm: 2.4748 | dt: 15856.19ms | tok/sec: 33065.20\n",
            "step   233 | loss: 0.897317 | lr 1.9636e-04 | norm: 2.5906 | dt: 15905.42ms | tok/sec: 32962.84\n",
            "step   234 | loss: 0.895500 | lr 1.9720e-04 | norm: 2.3156 | dt: 15878.44ms | tok/sec: 33018.86\n",
            "step   235 | loss: 0.863467 | lr 1.9804e-04 | norm: 2.2476 | dt: 15881.13ms | tok/sec: 33013.27\n",
            "step   236 | loss: 0.836910 | lr 1.9888e-04 | norm: 2.3255 | dt: 15881.13ms | tok/sec: 33013.26\n",
            "step   237 | loss: 0.850685 | lr 1.9972e-04 | norm: 2.3526 | dt: 15870.41ms | tok/sec: 33035.57\n",
            "step   238 | loss: 0.843406 | lr 2.0056e-04 | norm: 2.0183 | dt: 15889.63ms | tok/sec: 32995.60\n",
            "step   239 | loss: 0.827544 | lr 2.0140e-04 | norm: 2.2701 | dt: 15884.11ms | tok/sec: 33007.07\n",
            "step   240 | loss: 0.812783 | lr 2.0224e-04 | norm: 2.4065 | dt: 15877.82ms | tok/sec: 33020.15\n",
            "step   241 | loss: 0.835082 | lr 2.0308e-04 | norm: 2.7451 | dt: 15890.15ms | tok/sec: 32994.54\n",
            "step   242 | loss: 0.795443 | lr 2.0392e-04 | norm: 2.4385 | dt: 15896.02ms | tok/sec: 32982.34\n",
            "step   243 | loss: 0.760120 | lr 2.0476e-04 | norm: 2.2671 | dt: 15882.62ms | tok/sec: 33010.18\n",
            "step   244 | loss: 0.768518 | lr 2.0559e-04 | norm: 2.0368 | dt: 15865.21ms | tok/sec: 33046.40\n",
            "step   245 | loss: 0.747556 | lr 2.0643e-04 | norm: 2.1119 | dt: 15857.06ms | tok/sec: 33063.38\n",
            "step   246 | loss: 0.730364 | lr 2.0727e-04 | norm: 2.1909 | dt: 15875.43ms | tok/sec: 33025.12\n",
            "step   247 | loss: 0.734291 | lr 2.0811e-04 | norm: 2.3925 | dt: 15868.53ms | tok/sec: 33039.49\n",
            "step   248 | loss: 0.714383 | lr 2.0895e-04 | norm: 2.4387 | dt: 15861.61ms | tok/sec: 33053.89\n",
            "step   249 | loss: 0.715594 | lr 2.0979e-04 | norm: 2.1902 | dt: 15880.62ms | tok/sec: 33014.32\n",
            "rank 0 sample 0: Hello, I'm a language model, the brain for\n",
            "The Sym\n",
            "emerged, the\n",
            "the all of aphasia,\n",
            "that function of the\n",
            "rank 0 sample 1: Hello, I'm a language model, and recall from a large insult\n",
            "experimental two the midline of\n",
            "The central of a single of brain and the\n",
            "rank 0 sample 2: Hello, I'm a language model,\n",
            "the phrenology,\n",
            "from inter in the\n",
            "at that time.\n",
            "in the brain.\n",
            "The 2\n",
            "rank 0 sample 3: Hello, I'm a language model, the work of the\n",
            "the activity.\n",
            "who as aphasia,\n",
            "physicist, the discovery of\n",
            "speak\n",
            "step   250 | loss: 0.657251 | lr 2.1063e-04 | norm: 2.3647 | dt: 16739.16ms | tok/sec: 31321.05\n",
            "step   251 | loss: 0.629223 | lr 2.1147e-04 | norm: 2.0979 | dt: 15883.91ms | tok/sec: 33007.50\n",
            "step   252 | loss: 0.668467 | lr 2.1231e-04 | norm: 2.5395 | dt: 15877.41ms | tok/sec: 33021.00\n",
            "step   253 | loss: 0.642926 | lr 2.1315e-04 | norm: 1.8512 | dt: 15860.99ms | tok/sec: 33055.18\n",
            "step   254 | loss: 0.587623 | lr 2.1399e-04 | norm: 1.6745 | dt: 15871.51ms | tok/sec: 33033.27\n",
            "step   255 | loss: 0.585458 | lr 2.1483e-04 | norm: 1.8907 | dt: 15893.86ms | tok/sec: 32986.82\n",
            "step   256 | loss: 0.601845 | lr 2.1566e-04 | norm: 2.1461 | dt: 15879.01ms | tok/sec: 33017.67\n",
            "step   257 | loss: 0.576484 | lr 2.1650e-04 | norm: 2.0073 | dt: 15879.08ms | tok/sec: 33017.53\n",
            "step   258 | loss: 0.552893 | lr 2.1734e-04 | norm: 2.1516 | dt: 15869.35ms | tok/sec: 33037.78\n",
            "step   259 | loss: 0.541532 | lr 2.1818e-04 | norm: 2.1333 | dt: 15894.17ms | tok/sec: 32986.19\n",
            "step   260 | loss: 0.521943 | lr 2.1902e-04 | norm: 1.9264 | dt: 15866.03ms | tok/sec: 33044.70\n",
            "step   261 | loss: 0.488950 | lr 2.1986e-04 | norm: 1.9349 | dt: 15877.45ms | tok/sec: 33020.91\n",
            "step   262 | loss: 0.456961 | lr 2.2070e-04 | norm: 1.9315 | dt: 15880.66ms | tok/sec: 33014.24\n",
            "step   263 | loss: 0.485327 | lr 2.2154e-04 | norm: 2.3935 | dt: 15876.79ms | tok/sec: 33022.28\n",
            "step   264 | loss: 0.448582 | lr 2.2238e-04 | norm: 1.9795 | dt: 15848.10ms | tok/sec: 33082.08\n",
            "step   265 | loss: 0.469125 | lr 2.2322e-04 | norm: 2.2578 | dt: 15883.83ms | tok/sec: 33007.65\n",
            "step   266 | loss: 0.471379 | lr 2.2406e-04 | norm: 2.0495 | dt: 15884.81ms | tok/sec: 33005.61\n",
            "step   267 | loss: 0.417215 | lr 2.2490e-04 | norm: 1.9212 | dt: 15875.13ms | tok/sec: 33025.74\n",
            "step   268 | loss: 0.396124 | lr 2.2573e-04 | norm: 1.9571 | dt: 15867.14ms | tok/sec: 33042.38\n",
            "step   269 | loss: 0.415141 | lr 2.2657e-04 | norm: 2.0519 | dt: 15886.01ms | tok/sec: 33003.12\n",
            "step   270 | loss: 0.381402 | lr 2.2741e-04 | norm: 1.8859 | dt: 15880.06ms | tok/sec: 33015.49\n",
            "step   271 | loss: 0.357088 | lr 2.2825e-04 | norm: 1.9018 | dt: 15891.26ms | tok/sec: 32992.22\n",
            "step   272 | loss: 0.355471 | lr 2.2909e-04 | norm: 1.7192 | dt: 15871.33ms | tok/sec: 33033.66\n",
            "step   273 | loss: 0.337563 | lr 2.2993e-04 | norm: 1.7033 | dt: 15882.12ms | tok/sec: 33011.22\n",
            "step   274 | loss: 0.322332 | lr 2.3077e-04 | norm: 1.7456 | dt: 15860.38ms | tok/sec: 33056.46\n",
            "step   275 | loss: 0.311879 | lr 2.3161e-04 | norm: 1.7426 | dt: 15886.97ms | tok/sec: 33001.13\n",
            "step   276 | loss: 0.290963 | lr 2.3245e-04 | norm: 1.6409 | dt: 15888.98ms | tok/sec: 32996.95\n",
            "step   277 | loss: 0.281596 | lr 2.3329e-04 | norm: 1.6001 | dt: 15863.83ms | tok/sec: 33049.26\n",
            "step   278 | loss: 0.281967 | lr 2.3413e-04 | norm: 1.7572 | dt: 15864.90ms | tok/sec: 33047.04\n",
            "step   279 | loss: 0.274600 | lr 2.3497e-04 | norm: 1.8050 | dt: 15883.25ms | tok/sec: 33008.86\n",
            "step   280 | loss: 0.274827 | lr 2.3580e-04 | norm: 1.7874 | dt: 15895.89ms | tok/sec: 32982.61\n",
            "step   281 | loss: 0.250920 | lr 2.3664e-04 | norm: 1.5000 | dt: 15863.57ms | tok/sec: 33049.81\n",
            "step   282 | loss: 0.232912 | lr 2.3748e-04 | norm: 1.2938 | dt: 15885.35ms | tok/sec: 33004.51\n",
            "step   283 | loss: 0.248118 | lr 2.3832e-04 | norm: 1.5079 | dt: 15858.24ms | tok/sec: 33060.91\n",
            "step   284 | loss: 0.239671 | lr 2.3916e-04 | norm: 1.4398 | dt: 15884.45ms | tok/sec: 33006.36\n",
            "step   285 | loss: 0.241637 | lr 2.4000e-04 | norm: 1.6964 | dt: 15853.05ms | tok/sec: 33071.75\n",
            "step   286 | loss: 0.230987 | lr 2.4084e-04 | norm: 1.7434 | dt: 15883.64ms | tok/sec: 33008.05\n",
            "step   287 | loss: 0.209703 | lr 2.4168e-04 | norm: 1.3594 | dt: 15861.19ms | tok/sec: 33054.77\n",
            "step   288 | loss: 0.184702 | lr 2.4252e-04 | norm: 1.1256 | dt: 15896.97ms | tok/sec: 32980.38\n",
            "step   289 | loss: 0.225835 | lr 2.4336e-04 | norm: 1.6337 | dt: 15870.72ms | tok/sec: 33034.93\n",
            "step   290 | loss: 0.182707 | lr 2.4420e-04 | norm: 1.5538 | dt: 15881.88ms | tok/sec: 33011.71\n",
            "step   291 | loss: 0.175911 | lr 2.4503e-04 | norm: 1.2659 | dt: 15868.11ms | tok/sec: 33040.35\n",
            "step   292 | loss: 0.193566 | lr 2.4587e-04 | norm: 1.4387 | dt: 15895.34ms | tok/sec: 32983.76\n",
            "step   293 | loss: 0.166806 | lr 2.4671e-04 | norm: 1.3590 | dt: 15894.35ms | tok/sec: 32985.82\n",
            "step   294 | loss: 0.154549 | lr 2.4755e-04 | norm: 1.1576 | dt: 15879.56ms | tok/sec: 33016.54\n",
            "step   295 | loss: 0.149715 | lr 2.4839e-04 | norm: 1.1262 | dt: 15879.89ms | tok/sec: 33015.85\n",
            "step   296 | loss: 0.171954 | lr 2.4923e-04 | norm: 1.3379 | dt: 15907.72ms | tok/sec: 32958.09\n",
            "step   297 | loss: 0.148098 | lr 2.5007e-04 | norm: 1.1200 | dt: 15886.70ms | tok/sec: 33001.69\n",
            "step   298 | loss: 0.142970 | lr 2.5091e-04 | norm: 1.0683 | dt: 15909.61ms | tok/sec: 32954.18\n",
            "step   299 | loss: 0.138590 | lr 2.5175e-04 | norm: 1.0350 | dt: 15875.15ms | tok/sec: 33025.71\n",
            "step   300 | loss: 0.129530 | lr 2.5259e-04 | norm: 1.0499 | dt: 15904.64ms | tok/sec: 32964.47\n",
            "step   301 | loss: 0.121078 | lr 2.5343e-04 | norm: 1.0454 | dt: 15897.77ms | tok/sec: 32978.71\n",
            "step   302 | loss: 0.131714 | lr 2.5427e-04 | norm: 0.9686 | dt: 15876.20ms | tok/sec: 33023.52\n",
            "step   303 | loss: 0.131287 | lr 2.5510e-04 | norm: 1.1401 | dt: 15875.41ms | tok/sec: 33025.16\n",
            "step   304 | loss: 0.113923 | lr 2.5594e-04 | norm: 1.1003 | dt: 15881.61ms | tok/sec: 33012.26\n",
            "step   305 | loss: 0.127759 | lr 2.5678e-04 | norm: 1.1986 | dt: 15855.25ms | tok/sec: 33067.15\n",
            "step   306 | loss: 0.136259 | lr 2.5762e-04 | norm: 1.3661 | dt: 15895.25ms | tok/sec: 32983.94\n",
            "step   307 | loss: 0.110985 | lr 2.5846e-04 | norm: 1.0480 | dt: 15874.36ms | tok/sec: 33027.34\n",
            "step   308 | loss: 0.112439 | lr 2.5930e-04 | norm: 1.1968 | dt: 15884.60ms | tok/sec: 33006.05\n",
            "step   309 | loss: 0.110496 | lr 2.6014e-04 | norm: 1.1608 | dt: 15896.14ms | tok/sec: 32982.09\n",
            "step   310 | loss: 0.097701 | lr 2.6098e-04 | norm: 0.9295 | dt: 15891.35ms | tok/sec: 32992.03\n",
            "step   311 | loss: 0.102806 | lr 2.6182e-04 | norm: 1.0369 | dt: 15866.65ms | tok/sec: 33043.40\n",
            "step   312 | loss: 0.102413 | lr 2.6266e-04 | norm: 0.9655 | dt: 15879.96ms | tok/sec: 33015.71\n",
            "step   313 | loss: 0.091755 | lr 2.6350e-04 | norm: 0.9345 | dt: 15846.33ms | tok/sec: 33085.77\n",
            "step   314 | loss: 0.085856 | lr 2.6434e-04 | norm: 0.8526 | dt: 15897.55ms | tok/sec: 32979.17\n",
            "step   315 | loss: 0.092519 | lr 2.6517e-04 | norm: 0.9527 | dt: 15880.63ms | tok/sec: 33014.31\n",
            "step   316 | loss: 0.092935 | lr 2.6601e-04 | norm: 0.9612 | dt: 15884.47ms | tok/sec: 33006.33\n",
            "step   317 | loss: 0.093563 | lr 2.6685e-04 | norm: 0.8312 | dt: 15881.29ms | tok/sec: 33012.93\n",
            "step   318 | loss: 0.080705 | lr 2.6769e-04 | norm: 0.7626 | dt: 15894.42ms | tok/sec: 32985.65\n",
            "step   319 | loss: 0.077791 | lr 2.6853e-04 | norm: 0.8575 | dt: 15867.06ms | tok/sec: 33042.54\n",
            "step   320 | loss: 0.077917 | lr 2.6937e-04 | norm: 0.7851 | dt: 15882.23ms | tok/sec: 33010.97\n",
            "step   321 | loss: 0.070867 | lr 2.7021e-04 | norm: 0.7935 | dt: 15875.94ms | tok/sec: 33024.07\n",
            "step   322 | loss: 0.072774 | lr 2.7105e-04 | norm: 0.7218 | dt: 15855.71ms | tok/sec: 33066.20\n",
            "step   323 | loss: 0.066773 | lr 2.7189e-04 | norm: 0.7477 | dt: 15883.95ms | tok/sec: 33007.40\n",
            "step   324 | loss: 0.065187 | lr 2.7273e-04 | norm: 0.7167 | dt: 15882.18ms | tok/sec: 33011.09\n",
            "step   325 | loss: 0.063008 | lr 2.7357e-04 | norm: 0.6640 | dt: 15888.41ms | tok/sec: 32998.14\n",
            "step   326 | loss: 0.067585 | lr 2.7441e-04 | norm: 0.7240 | dt: 15875.22ms | tok/sec: 33025.56\n",
            "step   327 | loss: 0.066959 | lr 2.7524e-04 | norm: 0.7462 | dt: 15900.91ms | tok/sec: 32972.19\n",
            "step   328 | loss: 0.069245 | lr 2.7608e-04 | norm: 0.7985 | dt: 15911.85ms | tok/sec: 32949.52\n",
            "step   329 | loss: 0.069131 | lr 2.7692e-04 | norm: 0.8923 | dt: 15879.15ms | tok/sec: 33017.39\n",
            "step   330 | loss: 0.065043 | lr 2.7776e-04 | norm: 0.7804 | dt: 15884.60ms | tok/sec: 33006.06\n",
            "step   331 | loss: 0.063528 | lr 2.7860e-04 | norm: 0.7201 | dt: 15839.21ms | tok/sec: 33100.65\n",
            "step   332 | loss: 0.060643 | lr 2.7944e-04 | norm: 0.7320 | dt: 15893.20ms | tok/sec: 32988.20\n",
            "step   333 | loss: 0.055864 | lr 2.8028e-04 | norm: 0.7000 | dt: 15880.29ms | tok/sec: 33015.01\n",
            "step   334 | loss: 0.063903 | lr 2.8112e-04 | norm: 0.8511 | dt: 15903.42ms | tok/sec: 32967.00\n",
            "step   335 | loss: 0.065663 | lr 2.8196e-04 | norm: 0.8736 | dt: 15898.59ms | tok/sec: 32977.00\n",
            "step   336 | loss: 0.055790 | lr 2.8280e-04 | norm: 0.6623 | dt: 15896.30ms | tok/sec: 32981.76\n",
            "step   337 | loss: 0.066627 | lr 2.8364e-04 | norm: 0.7834 | dt: 15883.34ms | tok/sec: 33008.68\n",
            "step   338 | loss: 0.051504 | lr 2.8448e-04 | norm: 0.6561 | dt: 15877.98ms | tok/sec: 33019.82\n",
            "step   339 | loss: 0.053231 | lr 2.8531e-04 | norm: 0.6105 | dt: 15896.85ms | tok/sec: 32980.62\n",
            "step   340 | loss: 0.054335 | lr 2.8615e-04 | norm: 0.7841 | dt: 15890.61ms | tok/sec: 32993.57\n",
            "step   341 | loss: 0.050994 | lr 2.8699e-04 | norm: 0.7889 | dt: 15879.73ms | tok/sec: 33016.17\n",
            "step   342 | loss: 0.054527 | lr 2.8783e-04 | norm: 0.6226 | dt: 15892.08ms | tok/sec: 32990.52\n",
            "step   343 | loss: 0.054144 | lr 2.8867e-04 | norm: 0.6914 | dt: 15883.89ms | tok/sec: 33007.53\n",
            "step   344 | loss: 0.051377 | lr 2.8951e-04 | norm: 0.6429 | dt: 15893.79ms | tok/sec: 32986.97\n",
            "step   345 | loss: 0.052554 | lr 2.9035e-04 | norm: 0.6488 | dt: 15864.90ms | tok/sec: 33047.03\n",
            "step   346 | loss: 0.045821 | lr 2.9119e-04 | norm: 0.6104 | dt: 15878.09ms | tok/sec: 33019.58\n",
            "step   347 | loss: 0.042888 | lr 2.9203e-04 | norm: 0.5648 | dt: 15885.09ms | tok/sec: 33005.04\n",
            "step   348 | loss: 0.046128 | lr 2.9287e-04 | norm: 0.5778 | dt: 15855.23ms | tok/sec: 33067.19\n",
            "step   349 | loss: 0.043362 | lr 2.9371e-04 | norm: 0.5861 | dt: 15882.76ms | tok/sec: 33009.89\n",
            "step   350 | loss: 0.039511 | lr 2.9455e-04 | norm: 0.5758 | dt: 15882.27ms | tok/sec: 33010.90\n",
            "step   351 | loss: 0.042854 | lr 2.9538e-04 | norm: 0.5849 | dt: 15865.80ms | tok/sec: 33045.17\n",
            "step   352 | loss: 0.041620 | lr 2.9622e-04 | norm: 0.5010 | dt: 15870.53ms | tok/sec: 33035.31\n",
            "step   353 | loss: 0.036354 | lr 2.9706e-04 | norm: 0.4694 | dt: 15888.42ms | tok/sec: 32998.11\n",
            "step   354 | loss: 0.040376 | lr 2.9790e-04 | norm: 0.4945 | dt: 15892.54ms | tok/sec: 32989.56\n",
            "step   355 | loss: 0.035044 | lr 2.9874e-04 | norm: 0.4286 | dt: 15874.25ms | tok/sec: 33027.57\n",
            "step   356 | loss: 0.037564 | lr 2.9958e-04 | norm: 0.4748 | dt: 15898.94ms | tok/sec: 32976.28\n",
            "step   357 | loss: 0.035399 | lr 3.0042e-04 | norm: 0.4627 | dt: 15874.24ms | tok/sec: 33027.59\n",
            "step   358 | loss: 0.035349 | lr 3.0126e-04 | norm: 0.4053 | dt: 15870.18ms | tok/sec: 33036.05\n",
            "step   359 | loss: 0.034638 | lr 3.0210e-04 | norm: 0.4994 | dt: 15868.83ms | tok/sec: 33038.85\n",
            "step   360 | loss: 0.038163 | lr 3.0294e-04 | norm: 0.5780 | dt: 15888.46ms | tok/sec: 32998.04\n",
            "step   361 | loss: 0.034455 | lr 3.0378e-04 | norm: 0.5020 | dt: 15891.71ms | tok/sec: 32991.30\n",
            "step   362 | loss: 0.037661 | lr 3.0462e-04 | norm: 0.4833 | dt: 15865.37ms | tok/sec: 33046.05\n",
            "step   363 | loss: 0.036495 | lr 3.0545e-04 | norm: 0.4412 | dt: 15861.18ms | tok/sec: 33054.80\n",
            "step   364 | loss: 0.033729 | lr 3.0629e-04 | norm: 0.4469 | dt: 15892.08ms | tok/sec: 32990.53\n",
            "step   365 | loss: 0.032920 | lr 3.0713e-04 | norm: 0.4601 | dt: 15889.21ms | tok/sec: 32996.48\n",
            "step   366 | loss: 0.034870 | lr 3.0797e-04 | norm: 0.4860 | dt: 15889.90ms | tok/sec: 32995.04\n",
            "step   367 | loss: 0.033991 | lr 3.0881e-04 | norm: 0.4205 | dt: 15880.03ms | tok/sec: 33015.56\n",
            "step   368 | loss: 0.028945 | lr 3.0965e-04 | norm: 0.4027 | dt: 15879.17ms | tok/sec: 33017.35\n",
            "step   369 | loss: 0.034924 | lr 3.1049e-04 | norm: 0.4406 | dt: 15883.01ms | tok/sec: 33009.35\n",
            "step   370 | loss: 0.030880 | lr 3.1133e-04 | norm: 0.4264 | dt: 15886.42ms | tok/sec: 33002.27\n",
            "step   371 | loss: 0.029924 | lr 3.1217e-04 | norm: 0.3915 | dt: 15891.28ms | tok/sec: 32992.18\n",
            "step   372 | loss: 0.033757 | lr 3.1301e-04 | norm: 0.4383 | dt: 15851.46ms | tok/sec: 33075.06\n",
            "step   373 | loss: 0.028981 | lr 3.1385e-04 | norm: 0.4278 | dt: 15896.77ms | tok/sec: 32980.79\n",
            "step   374 | loss: 0.026004 | lr 3.1469e-04 | norm: 0.3481 | dt: 15882.85ms | tok/sec: 33009.69\n",
            "step   375 | loss: 0.033751 | lr 3.1552e-04 | norm: 0.4665 | dt: 15865.15ms | tok/sec: 33046.53\n",
            "step   376 | loss: 0.027737 | lr 3.1636e-04 | norm: 0.3903 | dt: 15883.19ms | tok/sec: 33008.98\n",
            "step   377 | loss: 0.030344 | lr 3.1720e-04 | norm: 0.4637 | dt: 15869.04ms | tok/sec: 33038.42\n",
            "step   378 | loss: 0.028919 | lr 3.1804e-04 | norm: 0.4134 | dt: 15887.68ms | tok/sec: 32999.65\n",
            "step   379 | loss: 0.035028 | lr 3.1888e-04 | norm: 0.4885 | dt: 15871.68ms | tok/sec: 33032.91\n",
            "step   380 | loss: 0.029301 | lr 3.1972e-04 | norm: 0.4639 | dt: 15875.16ms | tok/sec: 33025.68\n",
            "step   381 | loss: 0.028694 | lr 3.2056e-04 | norm: 0.3903 | dt: 15870.76ms | tok/sec: 33034.85\n",
            "step   382 | loss: 0.030398 | lr 3.2140e-04 | norm: 0.4953 | dt: 15857.70ms | tok/sec: 33062.04\n",
            "step   383 | loss: 0.031423 | lr 3.2224e-04 | norm: 0.4338 | dt: 15881.67ms | tok/sec: 33012.15\n",
            "step   384 | loss: 0.033901 | lr 3.2308e-04 | norm: 0.4528 | dt: 15854.54ms | tok/sec: 33068.63\n",
            "step   385 | loss: 0.030504 | lr 3.2392e-04 | norm: 0.4744 | dt: 15859.60ms | tok/sec: 33058.08\n",
            "step   386 | loss: 0.035920 | lr 3.2476e-04 | norm: 0.5916 | dt: 15899.87ms | tok/sec: 32974.37\n",
            "step   387 | loss: 0.036392 | lr 3.2559e-04 | norm: 0.6282 | dt: 15881.94ms | tok/sec: 33011.59\n",
            "step   388 | loss: 0.040328 | lr 3.2643e-04 | norm: 0.6967 | dt: 15860.56ms | tok/sec: 33056.08\n",
            "step   389 | loss: 0.045173 | lr 3.2727e-04 | norm: 0.8207 | dt: 15905.81ms | tok/sec: 32962.05\n",
            "step   390 | loss: 0.047891 | lr 3.2811e-04 | norm: 0.8916 | dt: 15891.97ms | tok/sec: 32990.75\n",
            "step   391 | loss: 0.053850 | lr 3.2895e-04 | norm: 1.1598 | dt: 15889.17ms | tok/sec: 32996.56\n",
            "step   392 | loss: 0.081745 | lr 3.2979e-04 | norm: 1.6871 | dt: 15883.54ms | tok/sec: 33008.25\n",
            "step   393 | loss: 0.132881 | lr 3.3063e-04 | norm: 2.7258 | dt: 15882.05ms | tok/sec: 33011.37\n",
            "step   394 | loss: 0.087492 | lr 3.3147e-04 | norm: 1.3270 | dt: 15895.22ms | tok/sec: 32983.99\n",
            "step   395 | loss: 0.055471 | lr 3.3231e-04 | norm: 0.7855 | dt: 15871.91ms | tok/sec: 33032.44\n",
            "step   396 | loss: 0.050621 | lr 3.3315e-04 | norm: 0.8318 | dt: 15862.43ms | tok/sec: 33052.19\n",
            "step   397 | loss: 0.044551 | lr 3.3399e-04 | norm: 0.6801 | dt: 15846.17ms | tok/sec: 33086.11\n",
            "step   398 | loss: 0.040161 | lr 3.3483e-04 | norm: 0.4982 | dt: 15881.46ms | tok/sec: 33012.59\n",
            "step   399 | loss: 0.041314 | lr 3.3566e-04 | norm: 0.5298 | dt: 15895.72ms | tok/sec: 32982.96\n",
            "step   400 | loss: 0.040332 | lr 3.3650e-04 | norm: 0.5180 | dt: 15875.79ms | tok/sec: 33024.38\n",
            "step   401 | loss: 0.033001 | lr 3.3734e-04 | norm: 0.4448 | dt: 15865.77ms | tok/sec: 33045.23\n",
            "step   402 | loss: 0.032575 | lr 3.3818e-04 | norm: 0.4498 | dt: 15881.32ms | tok/sec: 33012.87\n",
            "step   403 | loss: 0.035565 | lr 3.3902e-04 | norm: 0.4404 | dt: 15879.72ms | tok/sec: 33016.19\n",
            "step   404 | loss: 0.042651 | lr 3.3986e-04 | norm: 0.4771 | dt: 15884.06ms | tok/sec: 33007.18\n",
            "step   405 | loss: 0.033654 | lr 3.4070e-04 | norm: 0.4516 | dt: 15885.01ms | tok/sec: 33005.20\n",
            "step   406 | loss: 0.041602 | lr 3.4154e-04 | norm: 0.4787 | dt: 15876.54ms | tok/sec: 33022.81\n",
            "step   407 | loss: 0.031379 | lr 3.4238e-04 | norm: 0.4142 | dt: 15900.59ms | tok/sec: 32972.86\n",
            "step   408 | loss: 0.027766 | lr 3.4322e-04 | norm: 0.3388 | dt: 15889.28ms | tok/sec: 32996.33\n",
            "step   409 | loss: 0.033774 | lr 3.4406e-04 | norm: 0.4125 | dt: 15883.31ms | tok/sec: 33008.74\n",
            "step   410 | loss: 0.028231 | lr 3.4490e-04 | norm: 0.3510 | dt: 15883.46ms | tok/sec: 33008.42\n",
            "step   411 | loss: 0.028560 | lr 3.4573e-04 | norm: 0.3850 | dt: 15893.80ms | tok/sec: 32986.96\n",
            "step   412 | loss: 0.028425 | lr 3.4657e-04 | norm: 0.3565 | dt: 15891.99ms | tok/sec: 32990.71\n",
            "step   413 | loss: 0.030953 | lr 3.4741e-04 | norm: 0.4422 | dt: 15885.55ms | tok/sec: 33004.09\n",
            "step   414 | loss: 0.026326 | lr 3.4825e-04 | norm: 0.3350 | dt: 15882.14ms | tok/sec: 33011.18\n",
            "step   415 | loss: 0.029606 | lr 3.4909e-04 | norm: 0.3375 | dt: 15885.12ms | tok/sec: 33004.98\n",
            "step   416 | loss: 0.022497 | lr 3.4993e-04 | norm: 0.3061 | dt: 15901.20ms | tok/sec: 32971.59\n",
            "step   417 | loss: 0.023682 | lr 3.5077e-04 | norm: 0.3004 | dt: 15889.67ms | tok/sec: 32995.53\n",
            "step   418 | loss: 0.024360 | lr 3.5161e-04 | norm: 0.2963 | dt: 15850.56ms | tok/sec: 33076.94\n",
            "step   419 | loss: 0.026565 | lr 3.5245e-04 | norm: 0.3145 | dt: 15886.86ms | tok/sec: 33001.36\n",
            "step   420 | loss: 0.024545 | lr 3.5329e-04 | norm: 0.3144 | dt: 15877.05ms | tok/sec: 33021.74\n",
            "step   421 | loss: 0.023510 | lr 3.5413e-04 | norm: 0.3000 | dt: 15864.03ms | tok/sec: 33048.86\n",
            "step   422 | loss: 0.022446 | lr 3.5497e-04 | norm: 0.3402 | dt: 15845.48ms | tok/sec: 33087.54\n",
            "step   423 | loss: 0.025891 | lr 3.5580e-04 | norm: 0.3383 | dt: 15881.81ms | tok/sec: 33011.85\n",
            "step   424 | loss: 0.022313 | lr 3.5664e-04 | norm: 0.2987 | dt: 15901.71ms | tok/sec: 32970.55\n",
            "step   425 | loss: 0.021966 | lr 3.5748e-04 | norm: 0.2684 | dt: 15850.03ms | tok/sec: 33078.05\n",
            "step   426 | loss: 0.024080 | lr 3.5832e-04 | norm: 0.3181 | dt: 15877.17ms | tok/sec: 33021.50\n",
            "step   427 | loss: 0.022145 | lr 3.5916e-04 | norm: 0.3003 | dt: 15918.61ms | tok/sec: 32935.54\n",
            "step   428 | loss: 0.033028 | lr 3.6000e-04 | norm: 0.4770 | dt: 15893.76ms | tok/sec: 32987.04\n",
            "step   429 | loss: 0.028380 | lr 3.6084e-04 | norm: 0.3638 | dt: 15915.28ms | tok/sec: 32942.43\n",
            "step   430 | loss: 0.029377 | lr 3.6168e-04 | norm: 0.4565 | dt: 15886.54ms | tok/sec: 33002.02\n",
            "step   431 | loss: 0.023930 | lr 3.6252e-04 | norm: 0.3358 | dt: 15884.03ms | tok/sec: 33007.24\n",
            "step   432 | loss: 0.027232 | lr 3.6336e-04 | norm: 0.3709 | dt: 15906.04ms | tok/sec: 32961.57\n",
            "step   433 | loss: 0.031251 | lr 3.6420e-04 | norm: 0.4311 | dt: 15887.83ms | tok/sec: 32999.35\n",
            "step   434 | loss: 0.031314 | lr 3.6503e-04 | norm: 0.4280 | dt: 15865.08ms | tok/sec: 33046.67\n",
            "step   435 | loss: 0.028782 | lr 3.6587e-04 | norm: 0.3986 | dt: 15905.00ms | tok/sec: 32963.73\n",
            "step   436 | loss: 0.029592 | lr 3.6671e-04 | norm: 0.3855 | dt: 15883.27ms | tok/sec: 33008.82\n",
            "step   437 | loss: 0.029073 | lr 3.6755e-04 | norm: 0.3661 | dt: 15887.57ms | tok/sec: 32999.89\n",
            "step   438 | loss: 0.025047 | lr 3.6839e-04 | norm: 0.3847 | dt: 15890.12ms | tok/sec: 32994.60\n",
            "step   439 | loss: 0.028331 | lr 3.6923e-04 | norm: 0.3475 | dt: 15883.48ms | tok/sec: 33008.37\n",
            "step   440 | loss: 0.027554 | lr 3.7007e-04 | norm: 0.3689 | dt: 15877.96ms | tok/sec: 33019.85\n",
            "step   441 | loss: 0.027565 | lr 3.7091e-04 | norm: 0.4096 | dt: 15840.80ms | tok/sec: 33097.31\n",
            "step   442 | loss: 0.028097 | lr 3.7175e-04 | norm: 0.4120 | dt: 15850.74ms | tok/sec: 33076.56\n",
            "step   443 | loss: 0.028850 | lr 3.7259e-04 | norm: 0.4226 | dt: 15874.92ms | tok/sec: 33026.19\n",
            "step   444 | loss: 0.026525 | lr 3.7343e-04 | norm: 0.3714 | dt: 15900.01ms | tok/sec: 32974.07\n",
            "step   445 | loss: 0.033369 | lr 3.7427e-04 | norm: 0.4682 | dt: 15893.34ms | tok/sec: 32987.91\n",
            "step   446 | loss: 0.028105 | lr 3.7510e-04 | norm: 0.3552 | dt: 15899.41ms | tok/sec: 32975.32\n",
            "step   447 | loss: 0.028636 | lr 3.7594e-04 | norm: 0.4223 | dt: 15865.10ms | tok/sec: 33046.63\n",
            "step   448 | loss: 0.025894 | lr 3.7678e-04 | norm: 0.3516 | dt: 15893.27ms | tok/sec: 32988.05\n",
            "step   449 | loss: 0.030065 | lr 3.7762e-04 | norm: 0.3848 | dt: 15883.05ms | tok/sec: 33009.27\n",
            "step   450 | loss: 0.028456 | lr 3.7846e-04 | norm: 0.3325 | dt: 15875.90ms | tok/sec: 33024.14\n",
            "step   451 | loss: 0.023788 | lr 3.7930e-04 | norm: 0.2775 | dt: 15880.97ms | tok/sec: 33013.61\n",
            "step   452 | loss: 0.024148 | lr 3.8014e-04 | norm: 0.2979 | dt: 15848.27ms | tok/sec: 33081.72\n",
            "step   453 | loss: 0.022608 | lr 3.8098e-04 | norm: 0.3160 | dt: 15870.63ms | tok/sec: 33035.11\n",
            "step   454 | loss: 0.027222 | lr 3.8182e-04 | norm: 0.3595 | dt: 15878.22ms | tok/sec: 33019.32\n",
            "step   455 | loss: 0.024734 | lr 3.8266e-04 | norm: 0.3749 | dt: 15881.53ms | tok/sec: 33012.43\n",
            "step   456 | loss: 0.022549 | lr 3.8350e-04 | norm: 0.2914 | dt: 15863.75ms | tok/sec: 33049.44\n",
            "step   457 | loss: 0.023206 | lr 3.8434e-04 | norm: 0.3107 | dt: 15874.32ms | tok/sec: 33027.43\n",
            "step   458 | loss: 0.028256 | lr 3.8517e-04 | norm: 0.3498 | dt: 15904.07ms | tok/sec: 32965.64\n",
            "step   459 | loss: 0.028068 | lr 3.8601e-04 | norm: 0.3299 | dt: 15873.42ms | tok/sec: 33029.30\n",
            "step   460 | loss: 0.024603 | lr 3.8685e-04 | norm: 0.3415 | dt: 15849.91ms | tok/sec: 33078.30\n",
            "step   461 | loss: 0.023688 | lr 3.8769e-04 | norm: 0.3263 | dt: 15851.49ms | tok/sec: 33075.00\n",
            "step   462 | loss: 0.025283 | lr 3.8853e-04 | norm: 0.3390 | dt: 15869.74ms | tok/sec: 33036.96\n",
            "step   463 | loss: 0.025595 | lr 3.8937e-04 | norm: 0.3510 | dt: 15884.71ms | tok/sec: 33005.83\n",
            "step   464 | loss: 0.031637 | lr 3.9021e-04 | norm: 0.4415 | dt: 15857.70ms | tok/sec: 33062.04\n",
            "step   465 | loss: 0.024736 | lr 3.9105e-04 | norm: 0.3427 | dt: 15865.65ms | tok/sec: 33045.47\n",
            "step   466 | loss: 0.031478 | lr 3.9189e-04 | norm: 0.4230 | dt: 15872.36ms | tok/sec: 33031.52\n",
            "step   467 | loss: 0.028881 | lr 3.9273e-04 | norm: 0.3491 | dt: 15855.63ms | tok/sec: 33066.35\n",
            "step   468 | loss: 0.028932 | lr 3.9357e-04 | norm: 0.4525 | dt: 15905.26ms | tok/sec: 32963.19\n",
            "step   469 | loss: 0.025185 | lr 3.9441e-04 | norm: 0.3634 | dt: 15873.70ms | tok/sec: 33028.73\n",
            "step   470 | loss: 0.031894 | lr 3.9524e-04 | norm: 0.3784 | dt: 15909.52ms | tok/sec: 32954.36\n",
            "step   471 | loss: 0.027008 | lr 3.9608e-04 | norm: 0.3587 | dt: 15928.70ms | tok/sec: 32914.68\n",
            "step   472 | loss: 0.024873 | lr 3.9692e-04 | norm: 0.3487 | dt: 15882.58ms | tok/sec: 33010.26\n",
            "step   473 | loss: 0.030502 | lr 3.9776e-04 | norm: 0.3571 | dt: 15907.07ms | tok/sec: 32959.44\n",
            "step   474 | loss: 0.030014 | lr 3.9860e-04 | norm: 0.4292 | dt: 15900.16ms | tok/sec: 32973.77\n",
            "step   475 | loss: 0.030181 | lr 3.9944e-04 | norm: 0.4273 | dt: 15893.34ms | tok/sec: 32987.91\n",
            "step   476 | loss: 0.029351 | lr 4.0028e-04 | norm: 0.3810 | dt: 15874.55ms | tok/sec: 33026.95\n",
            "step   477 | loss: 0.031771 | lr 4.0112e-04 | norm: 0.4114 | dt: 15876.94ms | tok/sec: 33021.98\n",
            "step   478 | loss: 0.021545 | lr 4.0196e-04 | norm: 0.2640 | dt: 15883.58ms | tok/sec: 33008.17\n",
            "step   479 | loss: 0.026995 | lr 4.0280e-04 | norm: 0.3198 | dt: 15875.56ms | tok/sec: 33024.84\n",
            "step   480 | loss: 0.024334 | lr 4.0364e-04 | norm: 0.2585 | dt: 15850.83ms | tok/sec: 33076.38\n",
            "step   481 | loss: 0.027439 | lr 4.0448e-04 | norm: 0.3343 | dt: 15860.45ms | tok/sec: 33056.32\n",
            "step   482 | loss: 0.023856 | lr 4.0531e-04 | norm: 0.3022 | dt: 15881.20ms | tok/sec: 33013.13\n",
            "step   483 | loss: 0.022621 | lr 4.0615e-04 | norm: 0.2693 | dt: 15873.98ms | tok/sec: 33028.13\n",
            "step   484 | loss: 0.022646 | lr 4.0699e-04 | norm: 0.2806 | dt: 15873.87ms | tok/sec: 33028.36\n",
            "step   485 | loss: 0.021967 | lr 4.0783e-04 | norm: 0.2324 | dt: 15878.90ms | tok/sec: 33017.91\n",
            "step   486 | loss: 0.021621 | lr 4.0867e-04 | norm: 0.2645 | dt: 15887.04ms | tok/sec: 33000.99\n",
            "step   487 | loss: 0.021812 | lr 4.0951e-04 | norm: 0.2720 | dt: 15901.65ms | tok/sec: 32970.66\n",
            "step   488 | loss: 0.023340 | lr 4.1035e-04 | norm: 0.2874 | dt: 15881.41ms | tok/sec: 33012.69\n",
            "step   489 | loss: 0.021048 | lr 4.1119e-04 | norm: 0.2635 | dt: 15899.43ms | tok/sec: 32975.27\n",
            "step   490 | loss: 0.024165 | lr 4.1203e-04 | norm: 0.3293 | dt: 15878.32ms | tok/sec: 33019.12\n",
            "step   491 | loss: 0.021840 | lr 4.1287e-04 | norm: 0.3022 | dt: 15938.00ms | tok/sec: 32895.47\n",
            "step   492 | loss: 0.023685 | lr 4.1371e-04 | norm: 0.3260 | dt: 15925.74ms | tok/sec: 32920.80\n",
            "step   493 | loss: 0.021363 | lr 4.1455e-04 | norm: 0.2715 | dt: 15894.56ms | tok/sec: 32985.38\n",
            "step   494 | loss: 0.028285 | lr 4.1538e-04 | norm: 0.3219 | dt: 15946.88ms | tok/sec: 32877.16\n",
            "step   495 | loss: 0.019491 | lr 4.1622e-04 | norm: 0.2604 | dt: 15910.24ms | tok/sec: 32952.86\n",
            "step   496 | loss: 0.020656 | lr 4.1706e-04 | norm: 0.2517 | dt: 15922.01ms | tok/sec: 32928.51\n",
            "step   497 | loss: 0.021342 | lr 4.1790e-04 | norm: 0.3225 | dt: 15917.02ms | tok/sec: 32938.83\n",
            "step   498 | loss: 0.023308 | lr 4.1874e-04 | norm: 0.3398 | dt: 15940.31ms | tok/sec: 32890.70\n",
            "step   499 | loss: 0.025471 | lr 4.1958e-04 | norm: 0.3426 | dt: 15932.94ms | tok/sec: 32905.91\n",
            "rank 0 sample 0: Hello, I'm a language model,\n",
            "a search, Wernickes deepest r,\n",
            "rebrumis it involves some\n",
            "\n",
            "rank 0 sample 1: Hello, I'm a language model,\n",
            " neurologist\n",
            " and sensory data, a large\n",
            "and neural science\n",
            "many brain surgery for the relevant information represented\n",
            "\n",
            "rank 0 sample 2: Hello, I'm a language model, Wernickes\n",
            "the\n",
            "the part of\n",
            "\n",
            "the mechanisms that account for a\n",
            "\n",
            "the\n",
            "rank 0 sample 3: Hello, I'm a language model,\n",
            " neurologist\n",
            "a small fraction\n",
            "of electricalideals Western science. The new type and a\n",
            "step   500 | loss: 0.024837 | lr 4.2042e-04 | norm: 0.3646 | dt: 16420.69ms | tok/sec: 31928.51\n",
            "step   501 | loss: 0.024347 | lr 4.2126e-04 | norm: 0.3020 | dt: 15895.25ms | tok/sec: 32983.95\n",
            "step   502 | loss: 0.026242 | lr 4.2210e-04 | norm: 0.3394 | dt: 15876.21ms | tok/sec: 33023.50\n",
            "step   503 | loss: 0.026434 | lr 4.2294e-04 | norm: 0.3067 | dt: 15863.22ms | tok/sec: 33050.55\n",
            "step   504 | loss: 0.021004 | lr 4.2378e-04 | norm: 0.2359 | dt: 15884.50ms | tok/sec: 33006.26\n",
            "step   505 | loss: 0.020413 | lr 4.2462e-04 | norm: 0.2367 | dt: 15835.28ms | tok/sec: 33108.86\n",
            "step   506 | loss: 0.021976 | lr 4.2545e-04 | norm: 0.2880 | dt: 15817.44ms | tok/sec: 33146.19\n",
            "step   507 | loss: 0.018916 | lr 4.2629e-04 | norm: 0.2166 | dt: 15822.89ms | tok/sec: 33134.78\n",
            "step   508 | loss: 0.020213 | lr 4.2713e-04 | norm: 0.2485 | dt: 15884.27ms | tok/sec: 33006.74\n",
            "step   509 | loss: 0.021313 | lr 4.2797e-04 | norm: 0.2985 | dt: 15877.84ms | tok/sec: 33020.12\n",
            "step   510 | loss: 0.022460 | lr 4.2881e-04 | norm: 0.2843 | dt: 15912.39ms | tok/sec: 32948.41\n",
            "step   511 | loss: 0.023075 | lr 4.2965e-04 | norm: 0.2729 | dt: 15916.20ms | tok/sec: 32940.53\n",
            "step   512 | loss: 0.024368 | lr 4.3049e-04 | norm: 0.2795 | dt: 15920.36ms | tok/sec: 32931.93\n",
            "step   513 | loss: 0.027481 | lr 4.3133e-04 | norm: 0.3735 | dt: 15959.49ms | tok/sec: 32851.17\n",
            "step   514 | loss: 0.030655 | lr 4.3217e-04 | norm: 0.3973 | dt: 15975.08ms | tok/sec: 32819.12\n",
            "step   515 | loss: 0.029879 | lr 4.3301e-04 | norm: 0.3332 | dt: 15929.27ms | tok/sec: 32913.49\n",
            "step   516 | loss: 0.023662 | lr 4.3385e-04 | norm: 0.2751 | dt: 15901.02ms | tok/sec: 32971.96\n",
            "step   517 | loss: 0.033821 | lr 4.3469e-04 | norm: 0.4953 | dt: 15912.71ms | tok/sec: 32947.74\n",
            "step   518 | loss: 0.036199 | lr 4.3552e-04 | norm: 0.4063 | dt: 15950.53ms | tok/sec: 32869.64\n",
            "step   519 | loss: 0.029652 | lr 4.3636e-04 | norm: 0.3359 | dt: 15903.53ms | tok/sec: 32966.76\n",
            "step   520 | loss: 0.028744 | lr 4.3720e-04 | norm: 0.3362 | dt: 15876.63ms | tok/sec: 33022.63\n",
            "step   521 | loss: 0.024926 | lr 4.3804e-04 | norm: 0.3035 | dt: 15875.10ms | tok/sec: 33025.81\n",
            "step   522 | loss: 0.024694 | lr 4.3888e-04 | norm: 0.2825 | dt: 15867.28ms | tok/sec: 33042.08\n",
            "step   523 | loss: 0.022927 | lr 4.3972e-04 | norm: 0.2804 | dt: 15889.46ms | tok/sec: 32995.97\n",
            "step   524 | loss: 0.024768 | lr 4.4056e-04 | norm: 0.2581 | dt: 15859.31ms | tok/sec: 33058.69\n",
            "step   525 | loss: 0.022253 | lr 4.4140e-04 | norm: 0.2269 | dt: 15820.27ms | tok/sec: 33140.26\n",
            "step   526 | loss: 0.023362 | lr 4.4224e-04 | norm: 0.3053 | dt: 15863.29ms | tok/sec: 33050.39\n",
            "step   527 | loss: 0.020285 | lr 4.4308e-04 | norm: 0.2482 | dt: 15898.32ms | tok/sec: 32977.58\n",
            "step   528 | loss: 0.021704 | lr 4.4392e-04 | norm: 0.2435 | dt: 15894.97ms | tok/sec: 32984.53\n",
            "step   529 | loss: 0.023124 | lr 4.4476e-04 | norm: 0.2561 | dt: 15898.83ms | tok/sec: 32976.52\n",
            "step   530 | loss: 0.022470 | lr 4.4559e-04 | norm: 0.2336 | dt: 15895.51ms | tok/sec: 32983.39\n",
            "step   531 | loss: 0.021881 | lr 4.4643e-04 | norm: 0.2171 | dt: 15956.79ms | tok/sec: 32856.72\n",
            "step   532 | loss: 0.019266 | lr 4.4727e-04 | norm: 0.2360 | dt: 15932.77ms | tok/sec: 32906.28\n",
            "step   533 | loss: 0.018600 | lr 4.4811e-04 | norm: 0.2178 | dt: 15968.68ms | tok/sec: 32832.28\n",
            "step   534 | loss: 0.017922 | lr 4.4895e-04 | norm: 0.2046 | dt: 15921.18ms | tok/sec: 32930.22\n",
            "step   535 | loss: 0.016217 | lr 4.4979e-04 | norm: 0.2220 | dt: 15927.16ms | tok/sec: 32917.87\n",
            "step   536 | loss: 0.019663 | lr 4.5063e-04 | norm: 0.2580 | dt: 15946.99ms | tok/sec: 32876.92\n",
            "step   537 | loss: 0.020106 | lr 4.5147e-04 | norm: 0.2846 | dt: 15948.94ms | tok/sec: 32872.90\n",
            "step   538 | loss: 0.020627 | lr 4.5231e-04 | norm: 0.2472 | dt: 15905.78ms | tok/sec: 32962.10\n",
            "step   539 | loss: 0.020866 | lr 4.5315e-04 | norm: 0.2730 | dt: 15874.34ms | tok/sec: 33027.39\n",
            "step   540 | loss: 0.020416 | lr 4.5399e-04 | norm: 0.2550 | dt: 15872.32ms | tok/sec: 33031.60\n",
            "step   541 | loss: 0.024973 | lr 4.5483e-04 | norm: 0.3277 | dt: 15910.19ms | tok/sec: 32952.96\n",
            "step   542 | loss: 0.032111 | lr 4.5566e-04 | norm: 0.6022 | dt: 15862.57ms | tok/sec: 33051.90\n",
            "step   543 | loss: 0.162973 | lr 4.5650e-04 | norm: 3.8076 | dt: 15821.27ms | tok/sec: 33138.17\n",
            "step   544 | loss: 0.136778 | lr 4.5734e-04 | norm: 2.2703 | dt: 15892.73ms | tok/sec: 32989.18\n",
            "step   545 | loss: 0.040268 | lr 4.5818e-04 | norm: 0.6031 | dt: 15950.44ms | tok/sec: 32869.82\n",
            "step   546 | loss: 0.038696 | lr 4.5902e-04 | norm: 0.6197 | dt: 15960.11ms | tok/sec: 32849.89\n",
            "step   547 | loss: 0.079448 | lr 4.5986e-04 | norm: 2.0704 | dt: 15884.69ms | tok/sec: 33005.86\n",
            "step   548 | loss: 0.105089 | lr 4.6070e-04 | norm: 1.7401 | dt: 15867.23ms | tok/sec: 33042.19\n",
            "step   549 | loss: 0.107540 | lr 4.6154e-04 | norm: 2.4133 | dt: 15884.11ms | tok/sec: 33007.07\n",
            "step   550 | loss: 0.039427 | lr 4.6238e-04 | norm: 0.4244 | dt: 15860.81ms | tok/sec: 33055.56\n",
            "step   551 | loss: 0.046390 | lr 4.6322e-04 | norm: 0.5545 | dt: 15917.71ms | tok/sec: 32937.40\n",
            "step   552 | loss: 0.039453 | lr 4.6406e-04 | norm: 0.5478 | dt: 15886.43ms | tok/sec: 33002.25\n",
            "step   553 | loss: 0.027905 | lr 4.6490e-04 | norm: 0.3413 | dt: 15904.52ms | tok/sec: 32964.72\n",
            "step   554 | loss: 0.029691 | lr 4.6573e-04 | norm: 0.3334 | dt: 15927.22ms | tok/sec: 32917.74\n",
            "step   555 | loss: 0.020920 | lr 4.6657e-04 | norm: 0.2215 | dt: 15925.57ms | tok/sec: 32921.14\n",
            "step   556 | loss: 0.022708 | lr 4.6741e-04 | norm: 0.2350 | dt: 15907.79ms | tok/sec: 32957.94\n",
            "step   557 | loss: 0.021462 | lr 4.6825e-04 | norm: 0.1961 | dt: 15893.98ms | tok/sec: 32986.58\n",
            "step   558 | loss: 0.020924 | lr 4.6909e-04 | norm: 0.2236 | dt: 15940.70ms | tok/sec: 32889.89\n",
            "step   559 | loss: 0.019417 | lr 4.6993e-04 | norm: 0.2190 | dt: 15945.19ms | tok/sec: 32880.64\n",
            "step   560 | loss: 0.017930 | lr 4.7077e-04 | norm: 0.2191 | dt: 15919.33ms | tok/sec: 32934.06\n",
            "step   561 | loss: 0.017467 | lr 4.7161e-04 | norm: 0.1838 | dt: 15835.73ms | tok/sec: 33107.92\n",
            "step   562 | loss: 0.021552 | lr 4.7245e-04 | norm: 0.2913 | dt: 15868.30ms | tok/sec: 33039.95\n",
            "step   563 | loss: 0.021818 | lr 4.7329e-04 | norm: 0.1947 | dt: 15877.72ms | tok/sec: 33020.36\n",
            "step   564 | loss: 0.018697 | lr 4.7413e-04 | norm: 0.2511 | dt: 15822.05ms | tok/sec: 33136.55\n",
            "step   565 | loss: 0.020665 | lr 4.7497e-04 | norm: 0.2276 | dt: 15783.59ms | tok/sec: 33217.28\n",
            "step   566 | loss: 0.018608 | lr 4.7580e-04 | norm: 0.2336 | dt: 15858.91ms | tok/sec: 33059.53\n",
            "step   567 | loss: 0.016044 | lr 4.7664e-04 | norm: 0.1943 | dt: 15916.59ms | tok/sec: 32939.72\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-b385a738309b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# added after video, this field is also used by the forward pass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for step in range(max_steps):\n",
        "    t0 = time.time()\n",
        "    last_step = (step == max_steps - 1)\n",
        "\n",
        "    # once in a while generate from the model (except step 0, which is noise)\n",
        "    if ((step > 0 and step % 250 == 0) or last_step) and (not use_compile):\n",
        "        model.eval()\n",
        "        num_return_sequences = 4\n",
        "        max_length = 32\n",
        "        tokens = enc.encode(\"Hello, I'm a language model,\") # funny test\n",
        "        tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "        tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
        "        xgen = tokens.to(device)\n",
        "        sample_rng = torch.Generator(device=device)\n",
        "        sample_rng.manual_seed(42)\n",
        "        while xgen.size(1) < max_length:\n",
        "            # forward the model to get the logits\n",
        "            with torch.no_grad():\n",
        "                with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
        "                    logits, loss = model(xgen) # (B, T, vocab_size)\n",
        "                # take the logits at the last position\n",
        "                logits = logits[:, -1, :] # (B, vocab_size)\n",
        "                # get the probabilities\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                # do top-k sampling of 50 (huggingface pipeline default)\n",
        "                # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
        "                topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
        "                # select a token from the top-k probabilities\n",
        "                # note: multinomial does not demand the input to sum to 1\n",
        "                ix = torch.multinomial(topk_probs, 1, generator=sample_rng) # (B, 1)\n",
        "                # gather the corresponding indices\n",
        "                xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
        "                # append to the sequence\n",
        "                xgen = torch.cat((xgen, xcol), dim=1)\n",
        "        # print the generated text\n",
        "        for i in range(num_return_sequences):\n",
        "            tokens = xgen[i, :max_length].tolist()\n",
        "            decoded = enc.decode(tokens)\n",
        "            print(f\"rank {ddp_rank} sample {i}: {decoded}\")\n",
        "\n",
        "    # do one step of the optimization\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    loss_accum = 0.0\n",
        "    for micro_step in range(grad_accum_steps):\n",
        "        # added after video, this field is also used by the forward pass.\n",
        "        batch_idx, (X_batch, y_batch) = next(enumerate(train_loader))\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
        "            logits, loss = model(X_batch, y_batch)\n",
        "        # we have to scale the loss to account for gradient accumulation,\n",
        "        # because the gradients just add on each successive backward().\n",
        "        # addition of gradients corresponds to a SUM in the objective, but\n",
        "        # instead of a SUM we want MEAN. Scale the loss here so it comes out right\n",
        "        loss = loss / grad_accum_steps\n",
        "        loss_accum += loss.detach()\n",
        "        loss.backward()\n",
        "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_lr(step)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "    optimizer.step()\n",
        "    if device_type == \"cuda\":\n",
        "        torch.cuda.synchronize() # wait for the GPU to finish work\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0 # time difference in seconds\n",
        "    batch_size = 64\n",
        "    # tokens_processed = train_loader.B * train_loader.T * grad_accum_steps * ddp_world_size\n",
        "    tokens_processed = batch_size * seq_length * grad_accum_steps * ddp_world_size\n",
        "    tokens_per_sec = tokens_processed / dt\n",
        "    if loss_accum.item() < 0.0078:\n",
        "        print(\"we reached desireable loss === \", loss_accum.item())\n",
        "    if master_process:\n",
        "        print(f\"step {step:5d} | loss: {loss_accum.item():.6f} | lr {lr:.4e} | norm: {norm:.4f} | dt: {dt*1000:.2f}ms | tok/sec: {tokens_per_sec:.2f}\")\n",
        "        with open(log_file, \"a\") as f:\n",
        "            f.write(f\"{step} train {loss_accum.item():.6f}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sampling"
      ],
      "metadata": {
        "id": "C1RU2qEuBubV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = enc.encode(\"brain consists of how many parts:\")\n",
        "tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "num_return_sequences = 5\n",
        "\n",
        "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
        "sample_rng = torch.Generator(device=device)\n",
        "sample_rng.manual_seed(42)\n",
        "\n",
        "max_length = 120\n",
        "xgen = tokens.to(device)\n",
        "while xgen.size(1) < max_length:\n",
        "    # forward the model to get the logits\n",
        "    with torch.no_grad():\n",
        "        with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
        "            logits, loss = model(xgen) # (B, T, vocab_size)\n",
        "        # take the logits at the last position\n",
        "        last_logits = logits[:, -1, :] # (B, vocab_size)\n",
        "        # get the probabilities\n",
        "        probs = F.softmax(last_logits, dim=-1)\n",
        "        # do top-k sampling of 50 (huggingface pipeline default)\n",
        "        # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
        "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
        "        # select a token from the top-k probabilities\n",
        "        # note: multinomial does not demand the input to sum to 1\n",
        "        ix = torch.multinomial(topk_probs, 1, generator=sample_rng) # (B, 1)\n",
        "        # gather the corresponding indices\n",
        "        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
        "        # append to the sequence\n",
        "        xgen = torch.cat((xgen, xcol), dim=1)\n",
        "# print the generated text\n",
        "for i in range(num_return_sequences):\n",
        "    tokenss = xgen[i, :max_length].tolist()\n",
        "    decoded = enc.decode(tokenss)\n",
        "    print(f\"rank {ddp_rank} sample {i}: \\n {decoded}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "-EmNqaM2XTBs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0664654-db65-422b-a891-b46441f749a1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rank 0 sample 0: \n",
            " brain consists of how many parts: All perceptions, all volitions occupy\n",
            "the same seat in these (cerebrum, such as quintessentiallycerned with distinctive cognitive faculties in turn comprise distinct\n",
            "groups of neurons with distinctive connectivity and\n",
            "developmental origin. In the medulla, pons, midbrain,\n",
            "and diencephalon, neurons are often grouped in distinct clusters termed nuclei. The surface of the cerebrum and cerebellum consists of a large folded sheet\n",
            "brum and cerebellum consists of a large folded sheet\n",
            "of neurons called the cerebral\n",
            "\n",
            "rank 0 sample 1: \n",
            " brain consists of how many parts: the brain. That is, particu-\n",
            "uted in several regions of the brain. That is, particular brain regions are not fully responsible for specific\n",
            "lar brain regions are not fully responsible for specific\n",
            "mental faculties but instead are elementary processing\n",
            "units that together have a role. Perception, movement,\n",
            "language, thought, and memory are all made possible\n",
            "by the interlinkage of serial and parallel processing\n",
            "in discrete brain regionscomputational modules\n",
            "within these regions. As a result, damage to a single\n",
            "area need not result in the\n",
            "\n",
            "rank 0 sample 2: \n",
            " brain consists of how many parts: or neuronal pathway, cellular connectionists, particu-\n",
            "uted in several regions of the brain. That is, particular brain regions are not fully responsible for specific\n",
            "lar brain regions are not fully responsible for specific\n",
            "mental faculties but instead are elementary processing\n",
            "units that together have a role. Perception, movement,\n",
            "language, thought, and memory are all made possible\n",
            "by the interlinkage of serial and parallel processing\n",
            "in discrete brain regionscomputational modules\n",
            "within these regions. As a result, damage to a single\n",
            "area need not result\n",
            "\n",
            "rank 0 sample 3: \n",
            " brain consists of how many parts:\n",
            ", particu-\n",
            "uted in several regions of the brain. That is, particular brain regions are not fully responsible for specific\n",
            "lar brain regions are not fully responsible for specific\n",
            "mental faculties but instead are elementary processing\n",
            "units that together have a role. Perception, movement,\n",
            "language, thought, and memory are all made possible\n",
            "by the interlinkage of serial and parallel processing\n",
            "in discrete brain regionscomputational modules\n",
            "within these regions. As a result, damage to a single\n",
            "area need not result in the complete loss of a\n",
            "\n",
            "rank 0 sample 4: \n",
            " brain consists of how many parts: anatomy and treat certain genetic epilepsy disorders, then DNA sequencing and measurements of electrical properties of individual neurons\n",
            "might be sufficient to produce an effective therapy.\n",
            "If one is interested in learning, perception, and exploration, then an analysis of systems of circuits and brain\n",
            "regions is likely to be required.\n",
            "The goal of modern neural science is to integrate\n",
            "all of these specialized levels into a coherent science.\n",
            "The effort forces us to confront new questions. If mental processes can be localized to discrete brain regions,\n",
            "what is the relationship between the functions of those\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), '/content/model.pth')"
      ],
      "metadata": {
        "id": "3UbPapXcYqRp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6i6oTEloDQ1k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}