{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NanoGPT"
      ],
      "metadata": {
        "id": "O5-PENAR2qyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "! pip install tiktoken"
      ],
      "metadata": {
        "id": "2kSh9VgNmDQj"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import time\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import tiktoken\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "# from hellaswag import render_example, iterate_examples"
      ],
      "metadata": {
        "id": "2olmLzB12qXJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024 # max sequence length\n",
        "    vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
        "    n_layer: int = 12 # number of layers\n",
        "    n_head: int = 12 # number of heads\n",
        "    n_embd: int = 768 # embedding dimension"
      ],
      "metadata": {
        "id": "hV7Uc0ST22ai"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "        # regularization\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
        "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True) # flash attention\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "        # output projection\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "        self.gelu    = nn.GELU(approximate='tanh')\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # weight sharing scheme\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "        # init params\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            std = 0.02\n",
        "            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
        "                std *= (2 * self.config.n_layer) ** -0.5\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx is of shape (B, T)\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
        "        # forward the token and posisition embeddings\n",
        "        pos = torch.arange(0,T , dtype=torch.long, device=idx.device) # shape (T)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
        "        x = tok_emb + pos_emb\n",
        "        # forward the blocks of the transformer\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        # forward the final layernorm and the classifier\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "        # print(logits)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, device_type):\n",
        "        # start with all of the candidate parameters (that require grad)\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == \"cuda\"\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
        "        return optimizer\n"
      ],
      "metadata": {
        "id": "StDmdA8p3UxJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "print(f\"using device: {device}\")\n",
        "\n",
        "# added after video, pytorch can be serious about it's device vs. device_type distinction\n",
        "device_type = \"cuda\" if device.startswith(\"cuda\") else \"cpu\"\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(1337)\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "total_batch_size = 524288 # 2**19, ~0.5M, in number of tokens\n",
        "B = 64 # micro batch size\n",
        "T = 1024 # sequence length\n",
        "ddp_rank = 0\n",
        "ddp_world_size = 1\n",
        "master_process = True\n",
        "grad_accum_steps = total_batch_size // (B * T * ddp_world_size)\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "# create model\n",
        "model = GPT(GPTConfig(vocab_size=50304))\n",
        "# model = GPT.from_pretrained(\"gpt2\") # or init from OpenAI GPT-2\n",
        "model.to(device)\n",
        "use_compile = False # torch.compile interferes with HellaSwag eval and Generation. TODO fix\n",
        "if use_compile:\n",
        "    model = torch.compile(model)\n",
        "raw_model = model # always contains the \"raw\" unwrapped model\n",
        "\n",
        "max_lr = 6e-4\n",
        "min_lr = max_lr * 0.1\n",
        "warmup_steps = 715\n",
        "max_steps = 19073 # 19,073 steps is ~1 epoch, if data is 10B tokens and batch size 0.5M tokens\n",
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_steps:\n",
        "        return max_lr * (it+1) / warmup_steps\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > max_steps:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n",
        "    return min_lr + coeff * (max_lr - min_lr)\n",
        "\n",
        "# optimize!\n",
        "optimizer = raw_model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device_type=device_type)\n",
        "\n",
        "# create the log directory we will write checkpoints to and log to\n",
        "log_dir = \"log\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "log_file = os.path.join(log_dir, f\"log.txt\")\n",
        "with open(log_file, \"w\") as f: # open for writing to clear the file\n",
        "    pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eixaIaF14GOT",
        "outputId": "693232f1-7717-455a-c8e9-4f715a794bdc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "\n",
        "tokensq = enc.encode(text)\n",
        "\n",
        "with open('tokens.txt', 'w') as file:\n",
        "    file.write(' '.join(map(str, tokensq)))\n",
        "\n",
        "print(f\"Tokenized {len(tokensq)} tokens.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYQ4M6oddSKJ",
        "outputId": "b4d4479f-1511-439b-b4e6-550f144cde3f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized 338025 tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ShakespeareDataset(Dataset):\n",
        "    def __init__(self, file_path, seq_length):\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "        # Read the tokens from the file\n",
        "        with open(file_path, 'r') as file:\n",
        "            tokens = list(map(int, file.read().split()))\n",
        "\n",
        "        # Trim tokens to ensure they can be evenly divided into sequences\n",
        "        num_sequences = len(tokens) // seq_length\n",
        "        self.tokens = tokens[:num_sequences * seq_length]\n",
        "        # print(num_sequences * seq_length, len(tokens))\n",
        "        self.tokens_tensor = torch.tensor(self.tokens, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokens_tensor) - self.seq_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Return a single sequence and its target\n",
        "        return (\n",
        "            self.tokens_tensor[idx:idx+self.seq_length],\n",
        "            self.tokens_tensor[idx+1:idx+self.seq_length+1]  # shifted by one token for the target\n",
        "        )\n",
        "\n",
        "    def next_batch(self, batch_size):\n",
        "        # Create DataLoader to yield batches\n",
        "        return DataLoader(self, batch_size=batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "id": "1n5rAvXYnhsW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'tokens.txt'\n",
        "seq_length = 1024\n",
        "\n",
        "dloader = ShakespeareDataset(file_path, seq_length)\n",
        "\n",
        "batch_size = 5\n",
        "train_loader = dloader.next_batch(batch_size)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "CVt-SAlMnjPC"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the tokens from the file\n",
        "with open('tokens.txt', 'r') as file:\n",
        "    tokens = list(map(int, file.read().split()))\n",
        "total_tokens_in_dataset = len(tokens)\n",
        "max_steps = total_tokens_in_dataset // train_loader.batch_size\n",
        "max_steps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yu0aLOeNt6Tm",
        "outputId": "5557e408-9b21-4bf8-a725-70f5142c8d49"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "67605"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIsaQdw02gLl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "809f63a0-7721-4602-d467-820b7a568506"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step     0 | loss: 9.264258 | lr 8.3916e-07 | norm: 30.9816 | dt: 14786.22ms | tok/sec: 35457.87\n",
            "step     1 | loss: 9.105246 | lr 1.6783e-06 | norm: 28.9072 | dt: 14989.01ms | tok/sec: 34978.15\n",
            "step     2 | loss: 9.228601 | lr 2.5175e-06 | norm: 29.5660 | dt: 15239.94ms | tok/sec: 34402.25\n",
            "step     3 | loss: 9.089302 | lr 3.3566e-06 | norm: 26.3932 | dt: 15101.45ms | tok/sec: 34717.72\n",
            "step     4 | loss: 8.826705 | lr 4.1958e-06 | norm: 21.6630 | dt: 14915.79ms | tok/sec: 35149.86\n",
            "step     5 | loss: 8.511155 | lr 5.0350e-06 | norm: 16.0343 | dt: 14875.27ms | tok/sec: 35245.62\n",
            "step     6 | loss: 8.496334 | lr 5.8741e-06 | norm: 12.4647 | dt: 14886.25ms | tok/sec: 35219.61\n",
            "step     7 | loss: 8.392440 | lr 6.7133e-06 | norm: 13.6997 | dt: 14928.48ms | tok/sec: 35119.98\n",
            "step     8 | loss: 8.110153 | lr 7.5524e-06 | norm: 14.4477 | dt: 14961.35ms | tok/sec: 35042.84\n",
            "step     9 | loss: 8.219318 | lr 8.3916e-06 | norm: 12.0770 | dt: 14992.67ms | tok/sec: 34969.61\n",
            "step    10 | loss: 8.046194 | lr 9.2308e-06 | norm: 8.3040 | dt: 14999.38ms | tok/sec: 34953.98\n",
            "step    11 | loss: 7.944476 | lr 1.0070e-05 | norm: 10.9157 | dt: 15017.12ms | tok/sec: 34912.69\n",
            "step    12 | loss: 7.911753 | lr 1.0909e-05 | norm: 12.3789 | dt: 14962.50ms | tok/sec: 35040.14\n",
            "step    13 | loss: 7.607243 | lr 1.1748e-05 | norm: 8.8341 | dt: 14932.26ms | tok/sec: 35111.09\n",
            "step    14 | loss: 7.673508 | lr 1.2587e-05 | norm: 4.9331 | dt: 14891.46ms | tok/sec: 35207.30\n",
            "step    15 | loss: 7.564351 | lr 1.3427e-05 | norm: 6.3977 | dt: 14935.73ms | tok/sec: 35102.93\n",
            "step    16 | loss: 7.538072 | lr 1.4266e-05 | norm: 4.8021 | dt: 14966.36ms | tok/sec: 35031.09\n",
            "step    17 | loss: 7.297416 | lr 1.5105e-05 | norm: 3.7353 | dt: 14990.95ms | tok/sec: 34973.64\n",
            "step    18 | loss: 7.414932 | lr 1.5944e-05 | norm: 4.1248 | dt: 14961.63ms | tok/sec: 35042.18\n",
            "step    19 | loss: 7.350579 | lr 1.6783e-05 | norm: 3.5383 | dt: 14919.98ms | tok/sec: 35140.00\n",
            "step    20 | loss: 7.257075 | lr 1.7622e-05 | norm: 2.3128 | dt: 14895.60ms | tok/sec: 35197.52\n",
            "step    21 | loss: 7.190596 | lr 1.8462e-05 | norm: 3.6215 | dt: 14907.00ms | tok/sec: 35170.60\n",
            "step    22 | loss: 7.130167 | lr 1.9301e-05 | norm: 2.6464 | dt: 14945.22ms | tok/sec: 35080.64\n",
            "step    23 | loss: 7.138543 | lr 2.0140e-05 | norm: 2.5614 | dt: 14943.16ms | tok/sec: 35085.47\n",
            "step    24 | loss: 7.058018 | lr 2.0979e-05 | norm: 2.5510 | dt: 14930.49ms | tok/sec: 35115.25\n",
            "step    25 | loss: 6.937453 | lr 2.1818e-05 | norm: 1.7312 | dt: 14916.51ms | tok/sec: 35148.16\n",
            "step    26 | loss: 6.938341 | lr 2.2657e-05 | norm: 2.8338 | dt: 14918.58ms | tok/sec: 35143.28\n",
            "step    27 | loss: 6.787735 | lr 2.3497e-05 | norm: 3.4625 | dt: 14947.54ms | tok/sec: 35075.19\n",
            "step    28 | loss: 6.996665 | lr 2.4336e-05 | norm: 2.8726 | dt: 14978.35ms | tok/sec: 35003.05\n",
            "step    29 | loss: 6.910310 | lr 2.5175e-05 | norm: 2.1142 | dt: 14959.41ms | tok/sec: 35047.37\n",
            "step    30 | loss: 6.817886 | lr 2.6014e-05 | norm: 1.6373 | dt: 14887.60ms | tok/sec: 35216.43\n",
            "step    31 | loss: 6.610088 | lr 2.6853e-05 | norm: 2.9763 | dt: 14929.02ms | tok/sec: 35118.71\n",
            "step    32 | loss: 6.754073 | lr 2.7692e-05 | norm: 1.5022 | dt: 14947.53ms | tok/sec: 35075.22\n",
            "step    33 | loss: 6.746860 | lr 2.8531e-05 | norm: 1.7578 | dt: 14997.07ms | tok/sec: 34959.37\n",
            "step    34 | loss: 6.692537 | lr 2.9371e-05 | norm: 2.1941 | dt: 14951.31ms | tok/sec: 35066.37\n",
            "step    35 | loss: 6.802519 | lr 3.0210e-05 | norm: 2.0278 | dt: 14946.00ms | tok/sec: 35078.81\n",
            "step    36 | loss: 6.609485 | lr 3.1049e-05 | norm: 2.5386 | dt: 14889.24ms | tok/sec: 35212.55\n",
            "step    37 | loss: 6.727589 | lr 3.1888e-05 | norm: 1.4837 | dt: 14934.71ms | tok/sec: 35105.33\n",
            "step    38 | loss: 6.729376 | lr 3.2727e-05 | norm: 2.0804 | dt: 14953.50ms | tok/sec: 35061.23\n",
            "step    39 | loss: 6.655472 | lr 3.3566e-05 | norm: 2.2567 | dt: 14990.85ms | tok/sec: 34973.87\n",
            "step    40 | loss: 6.645325 | lr 3.4406e-05 | norm: 1.7213 | dt: 14960.40ms | tok/sec: 35045.05\n",
            "step    41 | loss: 6.557118 | lr 3.5245e-05 | norm: 1.6952 | dt: 14938.46ms | tok/sec: 35096.52\n",
            "step    42 | loss: 6.532876 | lr 3.6084e-05 | norm: 1.7692 | dt: 14887.20ms | tok/sec: 35217.36\n",
            "step    43 | loss: 6.531555 | lr 3.6923e-05 | norm: 1.9809 | dt: 14885.22ms | tok/sec: 35222.05\n",
            "step    44 | loss: 6.584561 | lr 3.7762e-05 | norm: 2.0315 | dt: 14947.91ms | tok/sec: 35074.34\n",
            "step    45 | loss: 6.446060 | lr 3.8601e-05 | norm: 2.9101 | dt: 14965.43ms | tok/sec: 35033.27\n",
            "step    46 | loss: 6.475044 | lr 3.9441e-05 | norm: 1.3077 | dt: 14944.67ms | tok/sec: 35081.95\n",
            "step    47 | loss: 6.423787 | lr 4.0280e-05 | norm: 1.8632 | dt: 14910.16ms | tok/sec: 35163.13\n",
            "step    48 | loss: 6.402931 | lr 4.1119e-05 | norm: 1.9479 | dt: 14911.30ms | tok/sec: 35160.44\n",
            "step    49 | loss: 6.420708 | lr 4.1958e-05 | norm: 2.1146 | dt: 14914.90ms | tok/sec: 35151.97\n",
            "step    50 | loss: 6.180577 | lr 4.2797e-05 | norm: 1.6677 | dt: 14967.03ms | tok/sec: 35029.53\n",
            "step    51 | loss: 6.391947 | lr 4.3636e-05 | norm: 1.4213 | dt: 14968.60ms | tok/sec: 35025.85\n",
            "step    52 | loss: 6.233081 | lr 4.4476e-05 | norm: 1.0324 | dt: 14941.38ms | tok/sec: 35089.65\n",
            "step    53 | loss: 6.425285 | lr 4.5315e-05 | norm: 2.3919 | dt: 14879.86ms | tok/sec: 35234.75\n",
            "step    54 | loss: 6.178895 | lr 4.6154e-05 | norm: 2.1604 | dt: 14911.91ms | tok/sec: 35159.00\n",
            "step    55 | loss: 6.273679 | lr 4.6993e-05 | norm: 1.6643 | dt: 14942.58ms | tok/sec: 35086.85\n",
            "step    56 | loss: 6.307699 | lr 4.7832e-05 | norm: 1.3358 | dt: 14990.61ms | tok/sec: 34974.44\n",
            "step    57 | loss: 6.142297 | lr 4.8671e-05 | norm: 3.2677 | dt: 14984.42ms | tok/sec: 34988.88\n",
            "step    58 | loss: 6.238428 | lr 4.9510e-05 | norm: 1.7257 | dt: 14921.16ms | tok/sec: 35137.22\n",
            "step    59 | loss: 6.150113 | lr 5.0350e-05 | norm: 1.6266 | dt: 14906.82ms | tok/sec: 35171.00\n",
            "step    60 | loss: 6.110945 | lr 5.1189e-05 | norm: 1.3199 | dt: 14890.24ms | tok/sec: 35210.17\n",
            "step    61 | loss: 6.184075 | lr 5.2028e-05 | norm: 3.4066 | dt: 14939.70ms | tok/sec: 35093.61\n",
            "step    62 | loss: 6.141257 | lr 5.2867e-05 | norm: 1.9030 | dt: 14920.52ms | tok/sec: 35138.73\n",
            "step    63 | loss: 6.058359 | lr 5.3706e-05 | norm: 2.2097 | dt: 14970.18ms | tok/sec: 35022.16\n",
            "step    64 | loss: 5.947363 | lr 5.4545e-05 | norm: 2.9535 | dt: 14978.38ms | tok/sec: 35002.99\n",
            "step    65 | loss: 6.092025 | lr 5.5385e-05 | norm: 2.6881 | dt: 14983.55ms | tok/sec: 34990.90\n",
            "step    66 | loss: 5.894362 | lr 5.6224e-05 | norm: 1.3155 | dt: 14981.17ms | tok/sec: 34996.46\n",
            "step    67 | loss: 5.939161 | lr 5.7063e-05 | norm: 2.1280 | dt: 14974.61ms | tok/sec: 35011.79\n",
            "step    68 | loss: 5.828632 | lr 5.7902e-05 | norm: 2.7961 | dt: 14973.77ms | tok/sec: 35013.77\n",
            "step    69 | loss: 6.044929 | lr 5.8741e-05 | norm: 1.5863 | dt: 14967.73ms | tok/sec: 35027.88\n",
            "step    70 | loss: 5.771423 | lr 5.9580e-05 | norm: 3.0098 | dt: 14952.08ms | tok/sec: 35064.56\n",
            "step    71 | loss: 5.943862 | lr 6.0420e-05 | norm: 2.1075 | dt: 14948.79ms | tok/sec: 35072.26\n",
            "step    72 | loss: 5.909050 | lr 6.1259e-05 | norm: 2.6879 | dt: 14973.96ms | tok/sec: 35013.31\n",
            "step    73 | loss: 5.951951 | lr 6.2098e-05 | norm: 1.5466 | dt: 14947.48ms | tok/sec: 35075.34\n",
            "step    74 | loss: 5.979403 | lr 6.2937e-05 | norm: 2.6293 | dt: 14962.21ms | tok/sec: 35040.81\n",
            "step    75 | loss: 5.875784 | lr 6.3776e-05 | norm: 1.9823 | dt: 14965.89ms | tok/sec: 35032.19\n",
            "step    76 | loss: 5.724669 | lr 6.4615e-05 | norm: 2.0130 | dt: 14957.22ms | tok/sec: 35052.51\n",
            "step    77 | loss: 5.740731 | lr 6.5455e-05 | norm: 3.5928 | dt: 14946.23ms | tok/sec: 35078.27\n",
            "step    78 | loss: 5.760841 | lr 6.6294e-05 | norm: 2.1568 | dt: 14971.02ms | tok/sec: 35020.20\n",
            "step    79 | loss: 5.806946 | lr 6.7133e-05 | norm: 2.6591 | dt: 14951.70ms | tok/sec: 35065.44\n",
            "step    80 | loss: 5.810343 | lr 6.7972e-05 | norm: 3.6751 | dt: 14969.42ms | tok/sec: 35023.95\n",
            "step    81 | loss: 5.503451 | lr 6.8811e-05 | norm: 1.7356 | dt: 14966.47ms | tok/sec: 35030.84\n",
            "step    82 | loss: 5.706901 | lr 6.9650e-05 | norm: 3.6645 | dt: 14959.20ms | tok/sec: 35047.87\n",
            "step    83 | loss: 5.554865 | lr 7.0490e-05 | norm: 3.7390 | dt: 14968.43ms | tok/sec: 35026.26\n",
            "step    84 | loss: 5.651356 | lr 7.1329e-05 | norm: 1.8439 | dt: 14971.43ms | tok/sec: 35019.23\n",
            "step    85 | loss: 5.723897 | lr 7.2168e-05 | norm: 5.0492 | dt: 14970.57ms | tok/sec: 35021.24\n",
            "step    86 | loss: 5.613191 | lr 7.3007e-05 | norm: 2.8056 | dt: 14983.52ms | tok/sec: 34990.99\n",
            "step    87 | loss: 5.681943 | lr 7.3846e-05 | norm: 5.5634 | dt: 14985.57ms | tok/sec: 34986.19\n",
            "step    88 | loss: 5.702796 | lr 7.4685e-05 | norm: 5.1564 | dt: 14967.09ms | tok/sec: 35029.40\n",
            "step    89 | loss: 5.565357 | lr 7.5524e-05 | norm: 2.5710 | dt: 14961.42ms | tok/sec: 35042.65\n",
            "step    90 | loss: 5.597447 | lr 7.6364e-05 | norm: 3.7968 | dt: 14970.21ms | tok/sec: 35022.08\n",
            "step    91 | loss: 5.537728 | lr 7.7203e-05 | norm: 2.6521 | dt: 14966.54ms | tok/sec: 35030.68\n",
            "step    92 | loss: 5.517549 | lr 7.8042e-05 | norm: 2.2426 | dt: 14947.32ms | tok/sec: 35075.71\n",
            "step    93 | loss: 5.612782 | lr 7.8881e-05 | norm: 2.9030 | dt: 14957.14ms | tok/sec: 35052.69\n",
            "step    94 | loss: 5.553274 | lr 7.9720e-05 | norm: 2.0513 | dt: 14964.51ms | tok/sec: 35035.44\n",
            "step    95 | loss: 5.432591 | lr 8.0559e-05 | norm: 2.6418 | dt: 14968.28ms | tok/sec: 35026.61\n",
            "step    96 | loss: 5.460807 | lr 8.1399e-05 | norm: 1.8116 | dt: 14951.16ms | tok/sec: 35066.70\n",
            "step    97 | loss: 5.458735 | lr 8.2238e-05 | norm: 3.0621 | dt: 14939.87ms | tok/sec: 35093.20\n",
            "step    98 | loss: 5.491849 | lr 8.3077e-05 | norm: 1.9862 | dt: 14956.51ms | tok/sec: 35054.17\n",
            "step    99 | loss: 5.265014 | lr 8.3916e-05 | norm: 2.0482 | dt: 14962.89ms | tok/sec: 35039.22\n",
            "step   100 | loss: 5.353996 | lr 8.4755e-05 | norm: 1.6195 | dt: 14945.03ms | tok/sec: 35081.08\n",
            "step   101 | loss: 5.236218 | lr 8.5594e-05 | norm: 1.4132 | dt: 14953.62ms | tok/sec: 35060.95\n",
            "step   102 | loss: 5.362916 | lr 8.6434e-05 | norm: 1.9679 | dt: 14952.68ms | tok/sec: 35063.15\n",
            "step   103 | loss: 5.313927 | lr 8.7273e-05 | norm: 1.7248 | dt: 14965.51ms | tok/sec: 35033.08\n",
            "step   104 | loss: 5.268876 | lr 8.8112e-05 | norm: 1.3097 | dt: 14961.46ms | tok/sec: 35042.56\n",
            "step   105 | loss: 5.371833 | lr 8.8951e-05 | norm: 1.0282 | dt: 14952.96ms | tok/sec: 35062.48\n",
            "step   106 | loss: 5.275409 | lr 8.9790e-05 | norm: 2.2470 | dt: 14959.16ms | tok/sec: 35047.96\n",
            "step   107 | loss: 5.065417 | lr 9.0629e-05 | norm: 1.8073 | dt: 14963.32ms | tok/sec: 35038.21\n",
            "step   108 | loss: 5.094555 | lr 9.1469e-05 | norm: 1.8105 | dt: 14941.08ms | tok/sec: 35090.38\n",
            "step   109 | loss: 5.065432 | lr 9.2308e-05 | norm: 2.4132 | dt: 14926.57ms | tok/sec: 35124.48\n",
            "step   110 | loss: 5.213481 | lr 9.3147e-05 | norm: 1.2930 | dt: 14923.17ms | tok/sec: 35132.47\n",
            "step   111 | loss: 5.099204 | lr 9.3986e-05 | norm: 3.1420 | dt: 14935.43ms | tok/sec: 35103.65\n",
            "step   112 | loss: 5.153148 | lr 9.4825e-05 | norm: 1.6052 | dt: 14956.83ms | tok/sec: 35053.43\n",
            "step   113 | loss: 5.158762 | lr 9.5664e-05 | norm: 2.3544 | dt: 14953.83ms | tok/sec: 35060.44\n",
            "step   114 | loss: 5.098340 | lr 9.6503e-05 | norm: 1.7405 | dt: 14950.80ms | tok/sec: 35067.56\n",
            "step   115 | loss: 5.069935 | lr 9.7343e-05 | norm: 1.4846 | dt: 14948.19ms | tok/sec: 35073.67\n",
            "step   116 | loss: 4.995872 | lr 9.8182e-05 | norm: 1.6047 | dt: 14962.86ms | tok/sec: 35039.28\n",
            "step   117 | loss: 5.041892 | lr 9.9021e-05 | norm: 1.6133 | dt: 14941.07ms | tok/sec: 35090.39\n",
            "step   118 | loss: 5.086423 | lr 9.9860e-05 | norm: 1.1581 | dt: 14967.47ms | tok/sec: 35028.50\n",
            "step   119 | loss: 4.958278 | lr 1.0070e-04 | norm: 1.3504 | dt: 14953.77ms | tok/sec: 35060.58\n",
            "step   120 | loss: 4.951624 | lr 1.0154e-04 | norm: 2.8245 | dt: 14978.35ms | tok/sec: 35003.05\n",
            "step   121 | loss: 4.969621 | lr 1.0238e-04 | norm: 1.5357 | dt: 14980.68ms | tok/sec: 34997.60\n",
            "step   122 | loss: 4.977191 | lr 1.0322e-04 | norm: 1.3765 | dt: 14974.29ms | tok/sec: 35012.55\n",
            "step   123 | loss: 4.954251 | lr 1.0406e-04 | norm: 1.7856 | dt: 14971.53ms | tok/sec: 35018.99\n",
            "step   124 | loss: 4.900812 | lr 1.0490e-04 | norm: 2.2487 | dt: 14963.59ms | tok/sec: 35037.58\n",
            "step   125 | loss: 4.893157 | lr 1.0573e-04 | norm: 1.6517 | dt: 14962.12ms | tok/sec: 35041.03\n",
            "step   126 | loss: 4.986313 | lr 1.0657e-04 | norm: 1.7359 | dt: 14956.48ms | tok/sec: 35054.25\n",
            "step   127 | loss: 4.944194 | lr 1.0741e-04 | norm: 1.3728 | dt: 14971.82ms | tok/sec: 35018.33\n",
            "step   128 | loss: 4.921224 | lr 1.0825e-04 | norm: 2.4531 | dt: 14969.51ms | tok/sec: 35023.73\n",
            "step   129 | loss: 4.823643 | lr 1.0909e-04 | norm: 1.4173 | dt: 15007.44ms | tok/sec: 34935.21\n",
            "step   130 | loss: 4.881391 | lr 1.0993e-04 | norm: 1.9101 | dt: 14983.24ms | tok/sec: 34991.63\n",
            "step   131 | loss: 4.658605 | lr 1.1077e-04 | norm: 2.4963 | dt: 14985.63ms | tok/sec: 34986.06\n",
            "step   132 | loss: 4.724747 | lr 1.1161e-04 | norm: 1.3833 | dt: 14990.94ms | tok/sec: 34973.66\n",
            "step   133 | loss: 4.831054 | lr 1.1245e-04 | norm: 2.2864 | dt: 14993.84ms | tok/sec: 34966.90\n",
            "step   134 | loss: 4.818978 | lr 1.1329e-04 | norm: 2.8134 | dt: 14988.00ms | tok/sec: 34980.51\n",
            "step   135 | loss: 4.800788 | lr 1.1413e-04 | norm: 1.9473 | dt: 14999.57ms | tok/sec: 34953.54\n",
            "step   136 | loss: 4.776629 | lr 1.1497e-04 | norm: 1.6410 | dt: 14995.41ms | tok/sec: 34963.22\n",
            "step   137 | loss: 4.807906 | lr 1.1580e-04 | norm: 2.4093 | dt: 14997.78ms | tok/sec: 34957.72\n",
            "step   138 | loss: 4.697767 | lr 1.1664e-04 | norm: 1.6434 | dt: 15002.09ms | tok/sec: 34947.67\n",
            "step   139 | loss: 4.673380 | lr 1.1748e-04 | norm: 1.4247 | dt: 15004.12ms | tok/sec: 34942.92\n",
            "step   140 | loss: 4.604863 | lr 1.1832e-04 | norm: 1.7509 | dt: 15001.03ms | tok/sec: 34950.13\n",
            "step   141 | loss: 4.432319 | lr 1.1916e-04 | norm: 1.5507 | dt: 15018.52ms | tok/sec: 34909.42\n",
            "step   142 | loss: 4.732064 | lr 1.2000e-04 | norm: 1.8138 | dt: 14995.91ms | tok/sec: 34962.06\n",
            "step   143 | loss: 4.785302 | lr 1.2084e-04 | norm: 2.8370 | dt: 14998.45ms | tok/sec: 34956.14\n",
            "step   144 | loss: 4.583229 | lr 1.2168e-04 | norm: 1.9022 | dt: 15001.95ms | tok/sec: 34948.00\n",
            "step   145 | loss: 4.640198 | lr 1.2252e-04 | norm: 2.2608 | dt: 14981.31ms | tok/sec: 34996.14\n",
            "step   146 | loss: 4.628218 | lr 1.2336e-04 | norm: 2.1442 | dt: 15006.35ms | tok/sec: 34937.75\n",
            "step   147 | loss: 4.650234 | lr 1.2420e-04 | norm: 1.2655 | dt: 14993.84ms | tok/sec: 34966.90\n",
            "step   148 | loss: 4.595243 | lr 1.2503e-04 | norm: 1.9879 | dt: 15005.75ms | tok/sec: 34939.13\n",
            "step   149 | loss: 4.594666 | lr 1.2587e-04 | norm: 2.4875 | dt: 14988.74ms | tok/sec: 34978.80\n",
            "step   150 | loss: 4.648339 | lr 1.2671e-04 | norm: 1.8287 | dt: 15002.76ms | tok/sec: 34946.10\n",
            "step   151 | loss: 4.589876 | lr 1.2755e-04 | norm: 2.5064 | dt: 14983.02ms | tok/sec: 34992.14\n",
            "step   152 | loss: 4.504156 | lr 1.2839e-04 | norm: 1.3180 | dt: 14979.22ms | tok/sec: 35001.03\n",
            "step   153 | loss: 4.514132 | lr 1.2923e-04 | norm: 1.4479 | dt: 14966.55ms | tok/sec: 35030.66\n",
            "step   154 | loss: 4.498473 | lr 1.3007e-04 | norm: 2.1294 | dt: 14987.13ms | tok/sec: 34982.56\n",
            "step   155 | loss: 4.434530 | lr 1.3091e-04 | norm: 2.3829 | dt: 14974.66ms | tok/sec: 35011.69\n",
            "step   156 | loss: 4.499959 | lr 1.3175e-04 | norm: 1.5209 | dt: 14981.63ms | tok/sec: 34995.39\n",
            "step   157 | loss: 4.470339 | lr 1.3259e-04 | norm: 2.3398 | dt: 14986.47ms | tok/sec: 34984.08\n",
            "step   158 | loss: 4.454102 | lr 1.3343e-04 | norm: 1.5751 | dt: 14986.04ms | tok/sec: 34985.10\n",
            "step   159 | loss: 4.350410 | lr 1.3427e-04 | norm: 1.8920 | dt: 14982.47ms | tok/sec: 34993.43\n",
            "step   160 | loss: 4.465065 | lr 1.3510e-04 | norm: 2.2502 | dt: 14979.56ms | tok/sec: 35000.24\n",
            "step   161 | loss: 4.326350 | lr 1.3594e-04 | norm: 1.3838 | dt: 14991.74ms | tok/sec: 34971.80\n",
            "step   162 | loss: 4.237880 | lr 1.3678e-04 | norm: 2.8456 | dt: 14987.72ms | tok/sec: 34981.16\n",
            "step   163 | loss: 4.407648 | lr 1.3762e-04 | norm: 1.3604 | dt: 15002.78ms | tok/sec: 34946.05\n",
            "step   164 | loss: 4.249203 | lr 1.3846e-04 | norm: 2.2758 | dt: 14993.42ms | tok/sec: 34967.87\n",
            "step   165 | loss: 4.427760 | lr 1.3930e-04 | norm: 2.1968 | dt: 14973.19ms | tok/sec: 35015.12\n",
            "step   166 | loss: 4.352004 | lr 1.4014e-04 | norm: 2.5143 | dt: 14964.03ms | tok/sec: 35036.55\n",
            "step   167 | loss: 4.344856 | lr 1.4098e-04 | norm: 1.8452 | dt: 15004.55ms | tok/sec: 34941.93\n",
            "step   168 | loss: 4.399004 | lr 1.4182e-04 | norm: 1.4682 | dt: 14981.46ms | tok/sec: 34995.78\n",
            "step   169 | loss: 4.312016 | lr 1.4266e-04 | norm: 1.6247 | dt: 14982.40ms | tok/sec: 34993.60\n",
            "step   170 | loss: 4.300271 | lr 1.4350e-04 | norm: 1.8876 | dt: 14997.09ms | tok/sec: 34959.31\n",
            "step   171 | loss: 4.318886 | lr 1.4434e-04 | norm: 1.8231 | dt: 15016.68ms | tok/sec: 34913.72\n",
            "step   172 | loss: 4.261375 | lr 1.4517e-04 | norm: 1.4490 | dt: 14996.46ms | tok/sec: 34960.79\n",
            "step   173 | loss: 4.287766 | lr 1.4601e-04 | norm: 1.6617 | dt: 14996.49ms | tok/sec: 34960.73\n",
            "step   174 | loss: 4.318885 | lr 1.4685e-04 | norm: 2.4207 | dt: 14987.15ms | tok/sec: 34982.51\n",
            "step   175 | loss: 4.307536 | lr 1.4769e-04 | norm: 1.2960 | dt: 15019.71ms | tok/sec: 34906.67\n",
            "step   176 | loss: 4.183893 | lr 1.4853e-04 | norm: 1.8037 | dt: 14989.69ms | tok/sec: 34976.58\n",
            "step   177 | loss: 4.140041 | lr 1.4937e-04 | norm: 1.9089 | dt: 14980.96ms | tok/sec: 34996.95\n",
            "step   178 | loss: 4.316004 | lr 1.5021e-04 | norm: 2.5045 | dt: 14999.62ms | tok/sec: 34953.41\n",
            "step   179 | loss: 4.202413 | lr 1.5105e-04 | norm: 1.6178 | dt: 14992.96ms | tok/sec: 34968.95\n",
            "step   180 | loss: 4.309839 | lr 1.5189e-04 | norm: 1.6401 | dt: 15006.29ms | tok/sec: 34937.88\n",
            "step   181 | loss: 4.266188 | lr 1.5273e-04 | norm: 2.3339 | dt: 14992.48ms | tok/sec: 34970.06\n",
            "step   182 | loss: 4.261971 | lr 1.5357e-04 | norm: 1.7345 | dt: 14995.24ms | tok/sec: 34963.62\n",
            "step   183 | loss: 4.238055 | lr 1.5441e-04 | norm: 2.2647 | dt: 14978.98ms | tok/sec: 35001.59\n",
            "step   184 | loss: 4.271520 | lr 1.5524e-04 | norm: 1.8485 | dt: 14994.99ms | tok/sec: 34964.21\n",
            "step   185 | loss: 4.147326 | lr 1.5608e-04 | norm: 1.6764 | dt: 14991.35ms | tok/sec: 34972.71\n",
            "step   186 | loss: 4.297178 | lr 1.5692e-04 | norm: 2.0249 | dt: 15002.74ms | tok/sec: 34946.16\n",
            "step   187 | loss: 4.172969 | lr 1.5776e-04 | norm: 1.9104 | dt: 14990.61ms | tok/sec: 34974.44\n",
            "step   188 | loss: 4.181094 | lr 1.5860e-04 | norm: 1.8945 | dt: 15012.16ms | tok/sec: 34924.22\n",
            "step   189 | loss: 4.184486 | lr 1.5944e-04 | norm: 1.8607 | dt: 14998.32ms | tok/sec: 34956.44\n",
            "step   190 | loss: 4.120342 | lr 1.6028e-04 | norm: 2.1141 | dt: 15003.76ms | tok/sec: 34943.77\n",
            "step   191 | loss: 4.074203 | lr 1.6112e-04 | norm: 1.7631 | dt: 14999.33ms | tok/sec: 34954.09\n",
            "step   192 | loss: 3.997109 | lr 1.6196e-04 | norm: 1.9559 | dt: 15019.17ms | tok/sec: 34907.92\n",
            "step   193 | loss: 4.028510 | lr 1.6280e-04 | norm: 1.6449 | dt: 15006.50ms | tok/sec: 34937.39\n",
            "step   194 | loss: 4.009676 | lr 1.6364e-04 | norm: 2.3942 | dt: 14983.94ms | tok/sec: 34989.98\n",
            "step   195 | loss: 4.110398 | lr 1.6448e-04 | norm: 2.0966 | dt: 14986.77ms | tok/sec: 34983.38\n",
            "step   196 | loss: 4.146789 | lr 1.6531e-04 | norm: 1.3349 | dt: 14995.39ms | tok/sec: 34963.28\n",
            "step   197 | loss: 4.164769 | lr 1.6615e-04 | norm: 2.4747 | dt: 14984.19ms | tok/sec: 34989.42\n",
            "step   198 | loss: 4.082644 | lr 1.6699e-04 | norm: 1.4763 | dt: 14980.32ms | tok/sec: 34998.46\n",
            "step   199 | loss: 4.127622 | lr 1.6783e-04 | norm: 1.5458 | dt: 14987.08ms | tok/sec: 34982.67\n",
            "step   200 | loss: 4.133954 | lr 1.6867e-04 | norm: 1.9142 | dt: 14990.89ms | tok/sec: 34973.78\n",
            "step   201 | loss: 4.065331 | lr 1.6951e-04 | norm: 2.4091 | dt: 14979.62ms | tok/sec: 35000.10\n",
            "step   202 | loss: 4.077734 | lr 1.7035e-04 | norm: 1.9793 | dt: 14967.79ms | tok/sec: 35027.74\n",
            "step   203 | loss: 4.193076 | lr 1.7119e-04 | norm: 1.7177 | dt: 14965.90ms | tok/sec: 35032.17\n",
            "step   204 | loss: 4.044453 | lr 1.7203e-04 | norm: 1.5589 | dt: 14976.69ms | tok/sec: 35006.94\n",
            "step   205 | loss: 3.997048 | lr 1.7287e-04 | norm: 1.9549 | dt: 15009.67ms | tok/sec: 34930.01\n",
            "step   206 | loss: 4.002150 | lr 1.7371e-04 | norm: 1.5852 | dt: 14975.31ms | tok/sec: 35010.16\n",
            "step   207 | loss: 4.046426 | lr 1.7455e-04 | norm: 1.5962 | dt: 14979.35ms | tok/sec: 35000.71\n",
            "step   208 | loss: 3.898564 | lr 1.7538e-04 | norm: 1.5744 | dt: 14980.40ms | tok/sec: 34998.26\n",
            "step   209 | loss: 4.007863 | lr 1.7622e-04 | norm: 2.2749 | dt: 14996.57ms | tok/sec: 34960.53\n",
            "step   210 | loss: 3.973684 | lr 1.7706e-04 | norm: 1.4843 | dt: 14988.26ms | tok/sec: 34979.92\n",
            "step   211 | loss: 4.046648 | lr 1.7790e-04 | norm: 2.7925 | dt: 14970.32ms | tok/sec: 35021.83\n",
            "step   212 | loss: 3.925836 | lr 1.7874e-04 | norm: 1.9046 | dt: 14987.47ms | tok/sec: 34981.76\n",
            "step   213 | loss: 3.982285 | lr 1.7958e-04 | norm: 1.7620 | dt: 15004.63ms | tok/sec: 34941.75\n",
            "step   214 | loss: 3.959305 | lr 1.8042e-04 | norm: 1.9303 | dt: 14963.91ms | tok/sec: 35036.84\n",
            "step   215 | loss: 3.934629 | lr 1.8126e-04 | norm: 1.4821 | dt: 14984.30ms | tok/sec: 34989.15\n",
            "step   216 | loss: 4.020334 | lr 1.8210e-04 | norm: 2.5600 | dt: 14971.07ms | tok/sec: 35020.07\n",
            "step   217 | loss: 3.937302 | lr 1.8294e-04 | norm: 2.1659 | dt: 14988.35ms | tok/sec: 34979.69\n",
            "step   218 | loss: 3.861632 | lr 1.8378e-04 | norm: 1.9388 | dt: 14975.59ms | tok/sec: 35009.51\n",
            "step   219 | loss: 3.839255 | lr 1.8462e-04 | norm: 1.4108 | dt: 14984.82ms | tok/sec: 34987.93\n",
            "step   220 | loss: 3.839589 | lr 1.8545e-04 | norm: 2.2949 | dt: 14990.10ms | tok/sec: 34975.61\n",
            "step   221 | loss: 3.894564 | lr 1.8629e-04 | norm: 2.2355 | dt: 15014.47ms | tok/sec: 34918.85\n",
            "step   222 | loss: 3.863751 | lr 1.8713e-04 | norm: 1.7189 | dt: 15002.13ms | tok/sec: 34947.58\n",
            "step   223 | loss: 3.910866 | lr 1.8797e-04 | norm: 2.4084 | dt: 15001.19ms | tok/sec: 34949.76\n",
            "step   224 | loss: 3.741373 | lr 1.8881e-04 | norm: 1.4999 | dt: 14996.80ms | tok/sec: 34959.99\n",
            "step   225 | loss: 3.781158 | lr 1.8965e-04 | norm: 1.7951 | dt: 15000.83ms | tok/sec: 34950.61\n",
            "step   226 | loss: 3.843992 | lr 1.9049e-04 | norm: 1.7268 | dt: 14990.51ms | tok/sec: 34974.65\n",
            "step   227 | loss: 3.768639 | lr 1.9133e-04 | norm: 1.9292 | dt: 14983.83ms | tok/sec: 34990.24\n",
            "step   228 | loss: 3.834165 | lr 1.9217e-04 | norm: 2.0924 | dt: 15000.41ms | tok/sec: 34951.58\n",
            "step   229 | loss: 3.716852 | lr 1.9301e-04 | norm: 2.0354 | dt: 15014.18ms | tok/sec: 34919.51\n",
            "step   230 | loss: 3.730838 | lr 1.9385e-04 | norm: 1.5456 | dt: 15023.76ms | tok/sec: 34897.25\n",
            "step   231 | loss: 3.829587 | lr 1.9469e-04 | norm: 1.9271 | dt: 14988.37ms | tok/sec: 34979.65\n",
            "step   232 | loss: 3.795046 | lr 1.9552e-04 | norm: 2.5341 | dt: 15000.90ms | tok/sec: 34950.44\n",
            "step   233 | loss: 3.789949 | lr 1.9636e-04 | norm: 2.1044 | dt: 15015.77ms | tok/sec: 34915.83\n",
            "step   234 | loss: 3.824756 | lr 1.9720e-04 | norm: 2.1043 | dt: 15003.38ms | tok/sec: 34944.65\n",
            "step   235 | loss: 3.638130 | lr 1.9804e-04 | norm: 2.3024 | dt: 15021.95ms | tok/sec: 34901.46\n",
            "step   236 | loss: 3.810342 | lr 1.9888e-04 | norm: 2.5133 | dt: 14997.85ms | tok/sec: 34957.53\n",
            "step   237 | loss: 3.755032 | lr 1.9972e-04 | norm: 1.8344 | dt: 15020.87ms | tok/sec: 34903.96\n",
            "step   238 | loss: 3.630550 | lr 2.0056e-04 | norm: 1.7674 | dt: 15010.80ms | tok/sec: 34927.40\n",
            "step   239 | loss: 3.564386 | lr 2.0140e-04 | norm: 2.6613 | dt: 15030.87ms | tok/sec: 34880.76\n",
            "step   240 | loss: 3.750438 | lr 2.0224e-04 | norm: 2.2951 | dt: 15012.83ms | tok/sec: 34922.67\n",
            "step   241 | loss: 3.857050 | lr 2.0308e-04 | norm: 2.2323 | dt: 15009.30ms | tok/sec: 34930.88\n",
            "step   242 | loss: 3.720297 | lr 2.0392e-04 | norm: 1.9372 | dt: 15019.95ms | tok/sec: 34906.11\n",
            "step   243 | loss: 3.771316 | lr 2.0476e-04 | norm: 1.5841 | dt: 15016.77ms | tok/sec: 34913.50\n"
          ]
        }
      ],
      "source": [
        "for step in range(max_steps):\n",
        "    t0 = time.time()\n",
        "    last_step = (step == max_steps - 1)\n",
        "\n",
        "    # once in a while generate from the model (except step 0, which is noise)\n",
        "    if ((step > 0 and step % 250 == 0) or last_step) and (not use_compile):\n",
        "        model.eval()\n",
        "        num_return_sequences = 4\n",
        "        max_length = 32\n",
        "        tokens = enc.encode(\"Hello, I'm a language model,\") # interesting start for shakespeare model :))\n",
        "        tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "        tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
        "        xgen = tokens.to(device)\n",
        "        sample_rng = torch.Generator(device=device)\n",
        "        sample_rng.manual_seed(42)\n",
        "        while xgen.size(1) < max_length:\n",
        "            # forward the model to get the logits\n",
        "            with torch.no_grad():\n",
        "                with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
        "                    logits, loss = model(xgen) # (B, T, vocab_size)\n",
        "                # take the logits at the last position\n",
        "                logits = logits[:, -1, :] # (B, vocab_size)\n",
        "                # get the probabilities\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                # do top-k sampling of 50 (huggingface pipeline default)\n",
        "                # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
        "                topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
        "                # select a token from the top-k probabilities\n",
        "                # note: multinomial does not demand the input to sum to 1\n",
        "                ix = torch.multinomial(topk_probs, 1, generator=sample_rng) # (B, 1)\n",
        "                # gather the corresponding indices\n",
        "                xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
        "                # append to the sequence\n",
        "                xgen = torch.cat((xgen, xcol), dim=1)\n",
        "        # print the generated text\n",
        "        for i in range(num_return_sequences):\n",
        "            tokens = xgen[i, :max_length].tolist()\n",
        "            decoded = enc.decode(tokens)\n",
        "            print(f\"rank {ddp_rank} sample {i}: {decoded}\")\n",
        "\n",
        "    # do one step of the optimization\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    loss_accum = 0.0\n",
        "    for micro_step in range(grad_accum_steps):\n",
        "        # added after video, this field is also used by the forward pass.\n",
        "        batch_idx, (X_batch, y_batch) = next(enumerate(train_loader))\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
        "            logits, loss = model(X_batch, y_batch)\n",
        "        # we have to scale the loss to account for gradient accumulation,\n",
        "        # because the gradients just add on each successive backward().\n",
        "        # addition of gradients corresponds to a SUM in the objective, but\n",
        "        # instead of a SUM we want MEAN. Scale the loss here so it comes out right\n",
        "        loss = loss / grad_accum_steps\n",
        "        loss_accum += loss.detach()\n",
        "        loss.backward()\n",
        "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_lr(step)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "    optimizer.step()\n",
        "    if device_type == \"cuda\":\n",
        "        torch.cuda.synchronize() # wait for the GPU to finish work\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0 # time difference in seconds\n",
        "    batch_size = 64\n",
        "    # tokens_processed = train_loader.B * train_loader.T * grad_accum_steps * ddp_world_size\n",
        "    tokens_processed = batch_size * seq_length * grad_accum_steps * ddp_world_size\n",
        "    tokens_per_sec = tokens_processed / dt\n",
        "    if loss_accum.item() < 0.0078:\n",
        "        print(\"we reached desireable loss === \", loss_accum.item())\n",
        "    if master_process:\n",
        "        print(f\"step {step:5d} | loss: {loss_accum.item():.6f} | lr {lr:.4e} | norm: {norm:.4f} | dt: {dt*1000:.2f}ms | tok/sec: {tokens_per_sec:.2f}\")\n",
        "        with open(log_file, \"a\") as f:\n",
        "            f.write(f\"{step} train {loss_accum.item():.6f}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sampling"
      ],
      "metadata": {
        "id": "C1RU2qEuBubV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = enc.encode(\"Second Citizen:\")\n",
        "tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "num_return_sequences = 5\n",
        "\n",
        "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
        "\n",
        "\n",
        "max_length = 40\n",
        "xgen = tokens.to(device)\n",
        "while xgen.size(1) < max_length:\n",
        "    # forward the model to get the logits\n",
        "    with torch.no_grad():\n",
        "        with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
        "            logits, loss = model(xgen) # (B, T, vocab_size)\n",
        "        # take the logits at the last position\n",
        "        last_logits = logits[:, -1, :] # (B, vocab_size)\n",
        "        # get the probabilities\n",
        "        probs = F.softmax(last_logits, dim=-1)\n",
        "        # do top-k sampling of 50 (huggingface pipeline default)\n",
        "        # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
        "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
        "        # select a token from the top-k probabilities\n",
        "        # note: multinomial does not demand the input to sum to 1\n",
        "        ix = torch.multinomial(topk_probs, 1) # (B, 1)\n",
        "        # gather the corresponding indices\n",
        "        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
        "        # append to the sequence\n",
        "        xgen = torch.cat((xgen, xcol), dim=1)\n",
        "# print the generated text\n",
        "for i in range(num_return_sequences):\n",
        "    tokenss = xgen[i, :max_length].tolist()\n",
        "    decoded = enc.decode(tokenss)\n",
        "    print(f\"rank {ddp_rank} sample {i}: {decoded}\")"
      ],
      "metadata": {
        "id": "-EmNqaM2XTBs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a474c1b7-6df6-4bb5-8668-922aac3f66f9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rank 0 sample 0: Second Citizen:\n",
            "What he cannot help in his nature, you account a\n",
            "vice in him. You must in repetition.\n",
            "What shouts are these? The other side o' the city\n",
            "is\n",
            "rank 0 sample 1: Second Citizen:\n",
            "What he cannot help in his nature, you account a\n",
            "vice in him. You must in no way say he is covetous.\n",
            "\n",
            "First Citizen:\n",
            "If I\n",
            "rank 0 sample 2: Second Citizen:\n",
            "Would you proceed especially against Caius Marcius?\n",
            "\n",
            "All:\n",
            "Against him first: he's a very dog to the commonalty.\n",
            "\n",
            "Second Citizen:\n",
            "\n",
            "rank 0 sample 3: Second Citizen:\n",
            "What he cannot help in his nature, you account a\n",
            "vice in him. You must in no way say he is covetous.\n",
            "\n",
            "First Citizen:\n",
            "If I\n",
            "rank 0 sample 4: Second Citizen:\n",
            "Would you proceed especially against Caius Marcius?\n",
            "\n",
            "All:\n",
            "Against him first: he's a very dog to the commonalty.\n",
            "\n",
            "Second Citizen:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3UbPapXcYqRp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}